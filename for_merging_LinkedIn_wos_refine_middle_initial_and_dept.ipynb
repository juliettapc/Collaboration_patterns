{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "import scipy\n",
    "import operator\n",
    "import difflib\n",
    "from IPython.core.display import display,HTML\n",
    "try:\n",
    "    import cPickle as pickle     #it is faster than pickle!\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "import unicodedata\n",
    "    \n",
    " \n",
    "    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # to make the notebook use the entire width of the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "################\n",
    "\n",
    "def convert_unicode_to_string(old_cadena):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        new_cadena=unicodedata.normalize('NFKD', old_cadena).encode('ascii','ignore')\n",
    "#         print old_cadena,type(old_cadena)\n",
    "#         print new_cadena,type(new_cadena)\n",
    "#         raw_input()\n",
    "        return new_cadena\n",
    "    except TypeError:  # if it is a string already\n",
    "#         print type(old_cadena)\n",
    "#         print old_cadena\n",
    "#         raw_input()\n",
    "        return old_cadena\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "\n",
    "def find_closest_match(cadena, lista):\n",
    "      #difflib.get_close_matches(a,b,3)   # this one gets you as many close matches as you want, in descending order of matching\n",
    "    ###   difflib.get_close_print \"wos univ:\", df_disamb_wos_test.University.unique()matches?   #### to read the manual of sth   \n",
    "        \n",
    "#     print cadena, lista    \n",
    "#     print type(cadena), type(lista)\n",
    "#     raw_input()\n",
    "\n",
    "\n",
    "    score=0.\n",
    "    match=None\n",
    "    \n",
    "    try:  # except when there is a NAN (which does not have a len())\n",
    "        \n",
    "        a=cadena\n",
    "        dict_index_score={}\n",
    "        \n",
    "        for i in range(len(lista)):    \n",
    "            b=lista[i]\n",
    "            score = difflib.SequenceMatcher(None,a, b).ratio()  \n",
    "            dict_index_score[i]=score\n",
    "\n",
    "  \n",
    "        sorted_dict = sorted(dict_index_score.items(), key=lambda x: x[1],reverse=True) # SORT DICT BY VALUE, IN DESCENDING ORDER     print \"sorted dictsorted_dict\",sorted_dict ## [(3, 0.9285714285714286), (6, 0.7586206896551724), (0, 0.6896551724137931), (5, 0.37037037037037035), (2, 0.35714285714285715), (4, 0.35714285714285715), (1, 0.23076923076923078)]\n",
    "\n",
    "        match=lista[sorted_dict[0][0]]\n",
    "        index=sorted_dict[0][0]\n",
    "        score=sorted_dict[0][1]                                                                        \n",
    "\n",
    "#         print \"\\nbest matching for \", cadena,\"is:\", match, \",   score:\"    , score\n",
    "#         raw_input()\n",
    "        \n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    return match, score\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "number_rows=1000000#\"All\" # or 1000000\n",
    "\n",
    "\n",
    "#path_cluster='/home/julia/Dropbox_collaborations/Data/WoS_data/Disambiguated_authors/'\n",
    "path_redbox='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/WoS_data/Disambiguated_authors/'\n",
    "\n",
    "string_num_rows=\"\"\n",
    "if  number_rows==\"All\":\n",
    "\n",
    "    df_disamb_wos_test=pd.read_csv(path_redbox+'wos_author_disambiguated_2000-2015_USA_processed.tsv', sep=\"\\t\")\n",
    "\n",
    "else:\n",
    "    string_num_rows=\"_\"+str(number_rows)\n",
    "    df_disamb_wos_test=pd.read_csv(path_redbox+'wos_author_disambiguated_2000-2015_USA_processed.tsv', sep=\"\\t\",nrows= number_rows)\n",
    "\n",
    "\n",
    "print \"done reading wos file\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_list_dept(string):\n",
    "    \n",
    "    new_string=None\n",
    "    try:\n",
    "        \n",
    "        new_string=string.replace(\", \",\",\").replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "    except AttributeError:  # except for nan \n",
    "        pass\n",
    "    \n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_disamb_wos_test['full_name'] = df_disamb_wos_test.full_name.apply(convert_unicode_to_string)\n",
    "df_disamb_wos_test['firstname'] = df_disamb_wos_test.firstname.apply(convert_unicode_to_string)\n",
    "df_disamb_wos_test['middle'] = df_disamb_wos_test.middle.apply(convert_unicode_to_string)\n",
    "df_disamb_wos_test['lastname'] = df_disamb_wos_test.lastname.apply(convert_unicode_to_string)\n",
    "\n",
    "\n",
    "df_disamb_wos_test['University'] = df_disamb_wos_test.apply(lambda row: row.University.replace(\", \",\",\").replace(\"[\",\"\").replace(\"]\",\"\").split(\",\"), axis=1)\n",
    "df_disamb_wos_test['Department'] = df_disamb_wos_test.Department.apply(create_list_dept)\n",
    "\n",
    "\n",
    "\n",
    "# example:  [University Michigan, University Michigan Hlth Syst]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_disamb_wos_test['Department'].iloc[112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_linkedin_redbox='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Vinu_University_Sheets/Improved_3Feb_and_more_emails_27Feb/'\n",
    "\n",
    "#path_linkedin_redbox='/home/julia/Dropbox_collaborations/Data/Vinu_University_Sheets/Improved_3Feb_and_more_emails_27Feb/All_linkedIn.pickle'\n",
    " \n",
    "df_linkedin_test=pd.read_pickle(path_linkedin_redbox+'All_linkedIn.pickle')\n",
    "\n",
    "\n",
    "print \"done reading linkedin file\"\n",
    "\n",
    "df_linkedin_test = df_linkedin_test.rename(columns={'Department(s)': 'Department', 'School/college': 'School_college'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_linkedin_test['full_name'] = df_linkedin_test.full_name.apply(convert_unicode_to_string)\n",
    "df_linkedin_test['firstname'] = df_linkedin_test.firstname.apply(convert_unicode_to_string)\n",
    "df_linkedin_test['middle'] = df_linkedin_test.middle.apply(convert_unicode_to_string)\n",
    "df_linkedin_test['lastname'] = df_linkedin_test.lastname.apply(convert_unicode_to_string)\n",
    "df_linkedin_test['dirty_firstname'] = df_linkedin_test.dirty_firstname.apply(convert_unicode_to_string)\n",
    "df_linkedin_test['dirty_lastname'] = df_linkedin_test.dirty_lastname.apply(convert_unicode_to_string)\n",
    "\n",
    "df_linkedin_test['University'] = df_linkedin_test.University.apply(convert_unicode_to_string)\n",
    "df_linkedin_test['Department'] = df_linkedin_test.Department.apply(convert_unicode_to_string)\n",
    "df_linkedin_test['School_college'] = df_linkedin_test.School_college.apply(convert_unicode_to_string)\n",
    "\n",
    "\n",
    "\n",
    "print \"WoS: \",df_disamb_wos_test.shape\n",
    "print \"LinkedIn: \",df_linkedin_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_linkedin_test['Department'].fillna(\"NAN\", inplace=True)\n",
    "df_disamb_wos_test['Department'].fillna(\"NAN\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_linkedin_test['middle'].fillna(\"NAN\", inplace=True)\n",
    "df_disamb_wos_test['middle'].fillna(\"NAN\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for item in df_disamb_wos_test.columns:\n",
    "#     print item\n",
    "\n",
    "df_disamb_wos_test.Department.iloc[16389]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "threshold  = 0.9  # to accept a matching of two university names\n",
    "\n",
    "\n",
    "# given two dataframes, i go over all names in one (linkedin)   and find  matches from the other\n",
    "\n",
    "master_dict_linkedin_index_wos_index={}\n",
    "cont=0\n",
    "print \"entering a loop of\",len(df_linkedin_test), \"iters (linkedin rows)......\\n\"\n",
    "\n",
    "\n",
    "cont_ambig=0\n",
    "# for row in tqdm_nofull_nametebook(df_linkedin_test.head(5000).iterrows()):       \n",
    "#for row in df_linkedin_test.head(1000).iterrows():       \n",
    "lista_linkedin_idx=[]    \n",
    "for row in tqdm_notebook(df_linkedin_test.iterrows()):       \n",
    "    cont +=1\n",
    "\n",
    "    linkedin_index=row[0]\n",
    "\n",
    "\n",
    "#     if linkedin_index ==100 or linkedin_index ==1000 or   linkedin_index ==5000 or linkedin_index ==10000 or linkedin_index == 50000 :\n",
    "#         print linkedin_index\n",
    "\n",
    "    lista_linkedin_idx.append(linkedin_index)\n",
    "\n",
    "    master_dict_linkedin_index_wos_index[linkedin_index]=[]\n",
    "    matching_dict_univ_linkedin_univ_wos={}     \n",
    "\n",
    "\n",
    "    univ_linkedin=row[1].University    \n",
    "    lastname_linkedin=row[1].lastname    \n",
    "    full_name_linkedin=row[1].full_name\n",
    "\n",
    "\n",
    "    lista_posibles_wos_index=[] \n",
    "\n",
    "\n",
    "    try:   # if the whole row is not just NANs    \n",
    "\n",
    "    #LINKEDIN:   dirty_firstname\tdirty_lastname\tfull_name\tfirstname\tmiddle\tlastname\n",
    "    #WOS:      firstname\tfirstname_initial\tmiddle\tlastname\tlist_author_names\tUniversity\tlist_years\tfull_name\n",
    "\n",
    "\n",
    "         first_name_linkedin=row[1].firstname\n",
    "\n",
    "\n",
    "         perfect_selection=df_disamb_wos_test[df_disamb_wos_test.full_name == full_name_linkedin]\n",
    "         if len( perfect_selection ) ==1:\n",
    "        #print \"perfect match on full name for linkedin_idx\", linkedin_index,\"  and wos_idx:\", perfect_selection.index\n",
    "        #raw_input()\n",
    "            select_df_wos_one_lastname = perfect_selection\n",
    "\n",
    "         else:                      \n",
    "\n",
    "         ### for a given last name, i only look at the wos rows with the same lastname\n",
    "            pre_select_df_wos_one_lastname=df_disamb_wos_test[df_disamb_wos_test.lastname == lastname_linkedin]\n",
    "\n",
    "            if len(pre_select_df_wos_one_lastname)>0:        \n",
    "                select_df_wos_one_lastname = pre_select_df_wos_one_lastname[pre_select_df_wos_one_lastname.firstname ==  first_name_linkedin]\n",
    "\n",
    "            # the selected df keeps the same indices from de original df !!!!!           \n",
    "            #print \"linkedin:\", linkedin_index, first_name_linkedin, lastname_linkedin, univ_linkedin\n",
    "\n",
    "\n",
    "            else:\n",
    "                select_df_wos_one_lastname = pre_select_df_wos_one_lastname\n",
    "\n",
    "\n",
    "#             print \"size pre-selection:\", len(pre_select_df_wos_one_lastname),  \"   size selection:\", len(select_df_wos_one_lastname)\n",
    "\n",
    "#             if len(select_df_wos_one_lastname)>0:\n",
    "#                 raw_input()\n",
    "\n",
    "\n",
    "         for fila in  select_df_wos_one_lastname.head(5000).iterrows():  # one row per author\n",
    "\n",
    "            list_univ_wos=fila[1].University                           \n",
    "            wos_index=fila[0]\n",
    "\n",
    "\n",
    "\n",
    "#             print type(univ_linkedin), univ_linkedin\n",
    "#             print type(list_univ_wos), list_univ_wos\n",
    "#             raw_input()\n",
    "\n",
    "\n",
    "            match, score= find_closest_match(univ_linkedin, list_univ_wos)\n",
    "\n",
    "#             print univ_linkedin, list_univ_wos\n",
    "#             print score\n",
    "#             raw_input()\n",
    "\n",
    "\n",
    "            if score > threshold:  #if the result of the matching is worth considering\n",
    "                matching_dict_univ_linkedin_univ_wos[univ_linkedin]=(match,  wos_index, score)\n",
    "                master_dict_linkedin_index_wos_index[linkedin_index].append(wos_index)   \n",
    "\n",
    "\n",
    "\n",
    "                lista_posibles_wos_index.append(wos_index)   \n",
    "\n",
    "\n",
    "         if len(lista_posibles_wos_index)>1:\n",
    "                cont_ambig +=1\n",
    "    except TypeError:   # for the few empty rows (all Nan)\n",
    "            pass\n",
    "\n",
    "\n",
    "print \"done with the big loop\\n\"\n",
    "\n",
    "\n",
    "print \"# of linkedin names with wos ambiguity:\", cont_ambig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_merge_linux='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Merged_LinkedIn_WoS/'\n",
    "\n",
    "\n",
    "string_fuzzy='NO'\n",
    "\n",
    "\n",
    "if string_fuzzy == \"YES\":\n",
    "# filename_pickle=path_merge_linux+\"master_dict_linkedin_index_wos_index.pickle\"\n",
    "    filename_pickle=path_merge_linux+\"master_dict_linkedin_index_wos_index_fuzzy_matching_name_and_univ_10000.pickle\"\n",
    "else:\n",
    "     filename_pickle=path_merge_linux+\"master_dict_linkedin_index_wos_index_exact_match_name_lastname.pickle\"\n",
    "\n",
    "\n",
    "\n",
    "master_dict_linkedin_index_wos_index=pd.read_pickle(filename_pickle)\n",
    "print len(master_dict_linkedin_index_wos_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "threshold_dept=0.5\n",
    "\n",
    "\n",
    "corrections_to_master_dict={}\n",
    "\n",
    "cont_disamb_found =0\n",
    "cont_disamb =0\n",
    "\n",
    "######## i go over the dict again, refining the matches using middle name and department info\n",
    "for linkedin_idx in master_dict_linkedin_index_wos_index:\n",
    "    \n",
    "    \n",
    "    corrections_to_master_dict[linkedin_idx]=master_dict_linkedin_index_wos_index[linkedin_idx]\n",
    "                    \n",
    "    \n",
    "    if len(master_dict_linkedin_index_wos_index[linkedin_idx]) > 1:\n",
    "        \n",
    "        cont_disamb +=1\n",
    "        dept_linkedin=df_linkedin_test.iloc[linkedin_idx].Department\n",
    "        middle_linkedin=df_linkedin_test.iloc[linkedin_idx].middle\n",
    "        \n",
    "        try:\n",
    "            for wos_indx in master_dict_linkedin_index_wos_index[linkedin_idx]:\n",
    "\n",
    "\n",
    "                list_dept_wos=df_disamb_wos_test.iloc[wos_indx].Department\n",
    "                middle_wos=df_disamb_wos_test.iloc[wos_indx].middle\n",
    "\n",
    "                flag_diff_middle=0                                                 \n",
    "                if middle_wos != np.nan and  middle_wos != \"NAN\" and middle_wos != \"\":\n",
    "                     if middle_linkedin != np.nan and  middle_linkedin != \"NAN\" and middle_linkedin != \"\":                \n",
    "                        if middle_linkedin != middle_wos:\n",
    "                            flag_diff_middle=1\n",
    "                           \n",
    "                            \n",
    "                           \n",
    "                        \n",
    "                        \n",
    "                            ####### for examples like:      stylianos <type 'str'>       s <type 'str'>\n",
    "                        if len(middle_wos)== 1    and    len(middle_linkedin) > 1:\n",
    "                                if middle_wos[0] == middle_linkedin[0]:\n",
    "                                    flag_diff_middle=0     \n",
    "                                    \n",
    "                        elif len(middle_linkedin)== 1    and    len(middle_wos) > 1:\n",
    "                                if middle_wos[0] == middle_linkedin[0]:\n",
    "                                    flag_diff_middle=0        \n",
    "                                    \n",
    "#                             if flag_diff_middle ==1:\n",
    "#                                  print middle_linkedin, type(middle_linkedin),\"     \", middle_wos ,type(middle_wos)\n",
    "                \n",
    "                         \n",
    "                \n",
    "                if flag_diff_middle ==0:\n",
    "                \n",
    "                    match, score = find_closest_match(dept_linkedin, list_dept_wos)\n",
    "\n",
    "                    if score >= threshold_dept:\n",
    "                        print \"\\n\\n\\n\",linkedin_idx, master_dict_linkedin_index_wos_index[linkedin_idx]\n",
    "                        print \"LINKEDIN:   \",df_linkedin_test.iloc[linkedin_idx].full_name, \" \", df_linkedin_test.iloc[linkedin_idx].Department, \" \", df_linkedin_test.iloc[linkedin_idx].University,\"\\n\"\n",
    "                        print \"----\", wos_indx, df_disamb_wos_test.iloc[wos_indx].full_name, \" \",df_disamb_wos_test.iloc[wos_indx].Department, \" \",df_disamb_wos_test.iloc[wos_indx].University\n",
    "\n",
    "\n",
    "                        print dept_linkedin,\"    \", match\n",
    "                        raw_input()\n",
    "\n",
    "\n",
    "                        corrections_to_master_dict[linkedin_idx]=[wos_indx]                                                            \n",
    "                        cont_disamb_found +=1\n",
    "            \n",
    "        except  IndexError:  # for when i use the full masterr dict but only a sample of Wos\n",
    "            pass\n",
    "\n",
    "print \"# matches found:\"        , cont_disamb_found, \"  out of:\", cont_disamb, float(cont_disamb_found)/cont_disamb\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path_merge_win='C:\\\\Users\\\\julietta\\\\Work\\\\Dropbox_studies\\\\Data\\\\Merged_LinkedIn_WoS\\\\'\n",
    "path_merge_redbox='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Merged_LinkedIn_WoS/'\n",
    "\n",
    "#path_merge_cluster='/home/julia/Dropbox_collaborations/Data/Merged_LinkedIn_WoS/'\n",
    "\n",
    "\n",
    "filename_pickle=path_merge_redbox+\"master_dict_linkedin_index_wos_index\"+string_num_rows+\".pickle\"    \n",
    "pickle.dump(master_dict_linkedin_index_wos_index, open(filename_pickle, 'wb'))\n",
    "print \"written:\",filename_pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_dict_linkedin_index_wos_index=corrections_to_master_dict\n",
    "\n",
    "\n",
    "print \"linkedin_idx  wos_idx\"\n",
    "cont_found=0\n",
    "cont_ambiguedad=0\n",
    "cont_no_match=0\n",
    "for llave in master_dict_linkedin_index_wos_index:\n",
    "    if len(master_dict_linkedin_index_wos_index[llave]) >0:\n",
    "        print llave, master_dict_linkedin_index_wos_index[llave]\n",
    "        cont_found +=1\n",
    "        if len(master_dict_linkedin_index_wos_index[llave]) >1:\n",
    "            cont_ambiguedad +=1 \n",
    "    else:\n",
    "        cont_no_match +=1\n",
    "\n",
    "print \"numb linkedin names: \",len(master_dict_linkedin_index_wos_index)\n",
    "print \"  numb. linkedin authors found: \", cont_found\n",
    "print \"      numb. linkedin authors found but with ambiguity: \", cont_ambiguedad\n",
    "print \"  numb. linkedin authors NOT found: \", cont_no_match\n",
    "print \n",
    "print \"size list linkedin idx\",len(lista_linkedin_idx), \"  unique:\", len(set(lista_linkedin_idx))\n",
    "print \"size master dict:\",len(master_dict_linkedin_index_wos_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_not_found_linkedin_lastnames=[]\n",
    "list_found_linkedin_lastnames=[]\n",
    "for linkedin_idx in master_dict_linkedin_index_wos_index:\n",
    "#print linkedin_idx, pickled_master_dict_linkedin_index_wos_index[linkedin_idx]\n",
    "    if len(master_dict_linkedin_index_wos_index[linkedin_idx]) == 0:\n",
    "        list_not_found_linkedin_lastnames.append(df_linkedin_test.iloc[linkedin_idx].lastname)\n",
    "    else:\n",
    "        list_found_linkedin_lastnames.append(df_linkedin_test.iloc[linkedin_idx].lastname)\n",
    "\n",
    "\n",
    "print \"tot # names on linkedin\", len(master_dict_linkedin_index_wos_index)   # 81444\n",
    "print \"# linkedin lastnmes NOT found on wos:\",len(list_not_found_linkedin_lastnames), \"  unique:\",len(set(list_not_found_linkedin_lastnames))  #    48840   unique: 29070  \n",
    "print \"# linkedin lastnames found on wos:\",len(list_found_linkedin_lastnames), \"  unique:\",len(set(list_found_linkedin_lastnames))   #  32604   unique: 18840 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_lastnames_in_wos=list(df_disamb_wos_test.lastname.values)\n",
    "\n",
    "\n",
    "\n",
    "print \"# wos lastnames:\",len(list_lastnames_in_wos), \"  unique:\",len(set(list_lastnames_in_wos))\n",
    "\n",
    "\n",
    "\n",
    "print \"\\noverlap between all wos lastnames and linkedin lastnames not found on wos:\", len(list( set(list_lastnames_in_wos)  &   set(list_not_found_linkedin_lastnames)   ))   #  20104    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linkedin_idx =489\n",
    "\n",
    "#if len(master_dict_linkedin_index_wos_index[linkedin_idx])>1:        \n",
    "print \"\\n\\nLINKEDIN idx:\",linkedin_idx, \"  \",df_linkedin_test.iloc[linkedin_idx].full_name.title() ,\" || \", df_linkedin_test.iloc[linkedin_idx].Current_Title,\" || \", df_linkedin_test.iloc[linkedin_idx].University,\" || \", df_linkedin_test.iloc[linkedin_idx].Department\n",
    "\n",
    "\n",
    "#for wos_indx in master_dict_linkedin_index_wos_index[linkedin_idx]:\n",
    "\n",
    "for wos_indx in corrections_to_master_dict[linkedin_idx]:\n",
    "\n",
    "    print \"\\n  ---wos_idx:\", wos_indx, \"WOS_disamb_ID:\", df_disamb_wos_test.iloc[wos_indx].author_id\n",
    "    print \"  \", df_disamb_wos_test.iloc[wos_indx].full_name\n",
    "    print \"  \",df_disamb_wos_test.iloc[wos_indx].University\n",
    "    print \"  \", df_disamb_wos_test.iloc[wos_indx].Department\n",
    "    print \"  \",df_disamb_wos_test.iloc[wos_indx].total_pubs , \"publ\"\n",
    "    print \"  \",df_disamb_wos_test.iloc[wos_indx].year\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#filename_output='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Merged_LinkedIn_WoS/checking_fuzzy_matcht_LinkedIn_WoS_with_repetitions.dat'\n",
    "filename_output_redbox='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Merged_LinkedIn_WoS/checking_matcht_LinkedIn_WoS'+string_num_rows+'.dat'\n",
    "\n",
    "output=open(filename_output_redbox,'wt')\n",
    "\n",
    "for linkedin_idx in master_dict_linkedin_index_wos_index:\n",
    "\n",
    "   \n",
    "    if len(master_dict_linkedin_index_wos_index[linkedin_idx])==1:        \n",
    "        try:\n",
    "            print >> output, \"\\n\\nLINKEDIN idx:\",linkedin_idx, \"  \",df_linkedin_test.iloc[linkedin_idx].full_name.title() ,\" || \", df_linkedin_test.iloc[linkedin_idx].Current_Title,\" || \", df_linkedin_test.iloc[linkedin_idx].University,\" || \", df_linkedin_test.iloc[linkedin_idx].Department\n",
    "        \n",
    "            print >> output ,\"\"\n",
    "\n",
    "\n",
    "            for wos_indx in master_dict_linkedin_index_wos_index[linkedin_idx]:\n",
    "   \n",
    "                print >> output , \"  ---wos_idx:\", wos_indx\n",
    "                print >> output ,\"  \", df_disamb_wos_test.iloc[wos_indx].full_name\n",
    "                print >> output ,\"  \",df_disamb_wos_test.iloc[wos_indx].University\n",
    "                print >> output ,\"  \", df_disamb_wos_test.iloc[wos_indx].Department\n",
    "                print >> output ,\"  \",df_disamb_wos_test.iloc[wos_indx].total_pubs , \"publ\"\n",
    "                print >> output ,\"  \",df_disamb_wos_test.iloc[wos_indx].year\n",
    "            \n",
    "        except UnicodeEncodeError:\n",
    "            print df_linkedin_test.iloc[linkedin_idx]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "output.close()\n",
    "print \"written:\",filename_output_redbox\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wos_with_new_column=df_disamb_wos_test.copy()\n",
    "linkedin_with_new_column=df_linkedin_test.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wos_with_new_column=wos_with_new_column.assign(merging_idx=np.nan)\n",
    "\n",
    "linkedin_with_new_column=linkedin_with_new_column.assign(merging_idx=np.nan)\n",
    "\n",
    "\n",
    "\n",
    "# wos_with_new_column.head()\n",
    "# linkedin_with_new_column.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cont_merging_idx=0\n",
    "for linkedin_idx in master_dict_linkedin_index_wos_index:\n",
    "\n",
    "    try:\n",
    "        wos_idx=master_dict_linkedin_index_wos_index[linkedin_idx][0] # OJO!!! for now i only consider one of the potentialy multiple wos_idx associated to a given linkedin idx\n",
    "        wos_with_new_column.set_value(wos_idx, 'merging_idx',int(cont_merging_idx) )\n",
    "# df.set_value('C', 'x', 10)     where:   index=['A','B','C']  and columns=['x','y']\n",
    "        linkedin_with_new_column.set_value(linkedin_idx, 'merging_idx', int(cont_merging_idx) )\n",
    "\n",
    "\n",
    "\n",
    "    except IndexError:\n",
    "        linkedin_with_new_column.set_value(linkedin_idx, 'merging_idx', 999999999 )\n",
    "\n",
    "\n",
    "#         print linkedin_idx, pickled_master_dict_linkedin_index_wos_index[linkedin_idx]\n",
    "#         raw_input()\n",
    "\n",
    "\n",
    "    cont_merging_idx +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "linkedin_with_new_column=linkedin_with_new_column.rename(columns = {'firstname':'firstname_linkedin'})\n",
    "linkedin_with_new_column=linkedin_with_new_column.rename(columns = {'lastname':'lastname_linkedin'})\n",
    "linkedin_with_new_column=linkedin_with_new_column.rename(columns = {'full_namename':'full_name_linkedin'})\n",
    "linkedin_with_new_column=linkedin_with_new_column.rename(columns = {'middle':'middle_linkedin'})\n",
    "\n",
    "linkedin_with_new_column=linkedin_with_new_column.rename(columns = {'University':'University_linkedin'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_merged = pd.merge(linkedin_with_new_column, wos_with_new_column, how='left',on='merging_idx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#path_merged_cluster='/home/julia/Dropbox_collaborations/Data/Merged_LinkedIn_WoS/'\n",
    "\n",
    "path_merged='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Merged_LinkedIn_WoS/'\n",
    "\n",
    "\n",
    "merged_file_name=path_merged+\"Merged_linkedin_wos\"+string_num_rows+\".tsv\"    \n",
    "\n",
    "df_merged.to_csv(merged_file_name, sep='\\t', encoding='utf-8')#, columns = list_headers)\n",
    "print \"csv done:\", merged_file_name\n",
    "\n",
    "\n",
    "df_merged.to_pickle(merged_file_name.split(\".tsv\")[0]+\".pickle\")\n",
    "print \"pickle done:\" ,merged_file_name.split(\".tsv\")[0]+\".pickle\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter(path_merged+\"Merged_linkedin_wos\"+string_num_rows+\".xlsx\", engine='xlsxwriter',options={'strings_to_urls': False})\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "df_merged.to_excel(writer, sheet_name='Sheet1')\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n",
    "\n",
    "print \"xlsx done:\", path_merged+\"Merged_linkedin_wos\"+string_num_rows+\".xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print \"Number of unique idx in merged df:\",len(df_merged.merging_idx.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def clean_wos_dept_info(string) : \n",
    "# #  example: '[Department Internal Med, Peninsula Med School, Great Ormond St Hospital Children Nhs Trust, Institute Ciencias Biol, \n",
    "# # Department Pathol, Calif Institute Quantitat Biomed Res, Department Pediat Surg, Center Clin Pharmacol & Therapeut, \n",
    "# # Mrc Center Inflammat Res, Department Biopharmaceut Sci & Pharmaceut Chem, Mrc Center Inflammat, Nis Center Excellence, \n",
    "# # Department Dermatol, Harborview Med Center, Department Resp Med, Department Clin Med, Rayne Labs, Papworth Hospital, \n",
    "# # Department Farmacol Aplicada, Department Clin & Expt Med, Center Inflammat Res, Department Ophthalmol, Institute Bioquim Med, \n",
    "# # Department Expt Pharmacol, School Biochem & Immunol, Department Appl Pharmacol, School Med, Department Genet, Genom Institute, \n",
    "# # Center Expt Therapeut & Reperfus Injury, No Carbon Res Labs, William Harvey Res Institute, Institute Biol, Center Biol Evaluat & Res, \n",
    "# # Department Biochem & Mol Biol, Eastchem School Chem, Keck School Med, Queens Med Res Institute, Addenbrookes Hospital, College Med & Vet Med]'\n",
    "\n",
    "#     try:\n",
    "              \n",
    "        \n",
    "#         new_string=string.replace(\"Med\", \"IMedicine\")\n",
    "#         new_string=string.replace(\"Pathol\", \"Pathology\")\n",
    "#         new_string=string.replace(\"Dermatol\", \"Dermatology\")\n",
    "#         new_string=string.replace(\"Genet\", \"Genetics\")\n",
    "#         new_string=string.replace(\"Biol\", \"Biology\")\n",
    "#         new_string=string.replace(\"Chem\", \"Chemistry\")\n",
    "#         new_string=string.replace(\"Expt\", \"Experimental\")\n",
    "#         new_string=string.replace(\"Dev\", \"Develomental\")\n",
    "#         new_string=string.replace(\"Canc \", \"Cancer \")\n",
    "#         new_string=string.replace(\"Molec\", \"Molecular\")\n",
    "#         new_string=string.replace(\"Sci\", \"Science\")\n",
    "#         new_string=string.replace(\"Hlth\", \"Health\")\n",
    "#         new_string=string.replace(\"Biochem\", \"Biochemistry\")\n",
    "#         new_string=string.replace(\"Pharmacol\", \"Pharmacology\")\n",
    "#         new_string=string.replace(\"Quantitat\", \"Quantitative\")\n",
    "#         new_string=string.replace(\"Res\", \"Research\")\n",
    "#         new_string=string.replace(\"Surg\", \"Surgery\")\n",
    "#         new_string=string.replace(\"Genet\", \"Genetics\")\n",
    "#         new_string=string.replace(\"Radiol\", \"Radiology\")\n",
    "#         new_string=string.replace(\"Engn\", \"Engineering\")\n",
    "#         new_string=string.replace(\"Neurosci\", \"Neuroscience\")Appl\n",
    "#         new_string=string.replace(\"Phys\", \"Physics\")\n",
    "#         new_string=string.replace(\"Nucl\", \"Nuclear\")\n",
    "#         new_string=string.replace(\"Astron\", \"Astronomy\")\n",
    "#         new_string=string.replace(\"Technol\", \"Technology\")\n",
    "#         new_string=string.replace(\"Oceanol\", \"Oceanology\")\n",
    "#         new_string=string.replace(\"Microbiol\", \"Microbiology\")\n",
    "#         new_string=string.replace(\"Environm\", \"Environmental\")\n",
    "#         new_string=string.replace(\"Elect\", \"Electrical\")\n",
    "#         new_string=string.replace(\"Comp\", \"Computer\")Anesthesiol\n",
    "#         new_string=string.replace(\"Mat\", \"Material\")\n",
    "#         new_string=string.replace(\"Crystallog\", \"Crystallography\")\n",
    "#         new_string=string.replace(\"Anesthesiol\", \"Anesthesiology\")\n",
    "#         new_string=string.replace(\"Oncol\", \"Oncology\")\n",
    "#         new_string=string.replace(\"Physiol\", \"Physiology\")\n",
    "#         new_string=string.replace(\"Pharmacol\", \"Pharmacology\")\n",
    "#         new_string=string.replace(\"Appl\", \"Apply\")\n",
    "#         new_string=string.replace(\"Vet\", \"Veterinarian\")\n",
    "#         new_string=string.replace(\"Sociol\", \"Sociology\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "#         new_string=string.replace(\"\", \"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#         return new_string\n",
    "#     except:\n",
    "#         return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
