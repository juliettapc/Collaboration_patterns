{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt   \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "import scipy\n",
    "import operator\n",
    "import difflib\n",
    "import math\n",
    "\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # to make the notebook use the entire width of the browser\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle     #it is faster than pickle!\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "import unicodedata\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import seaborn as sns   ### https://seaborn.pydata.org/tutorial/categorical.html\n",
    "import time  \n",
    "\n",
    "\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "# i only need my credentials if i want to plot online --- and send plots to server (limits per day apply!)\n",
    "#import plotly.tools as tls\n",
    "#tls.set_credentials_file(username='juliettapc', api_key='deyNIvtOoDZ5PLmrHlhd')  # my plotly account credentials\n",
    "\n",
    "\n",
    "import pygraphviz\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## to be able to plot offline (without sending the plots to the plotly server every time)\n",
    "import plotly.offline as offline\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "################\n",
    "\n",
    "\n",
    "\n",
    "##### for getting geolocation data  and to calculate distance between two geolocations\n",
    "import requests\n",
    "import json\n",
    "import geopy.distance   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "input_file = 'Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_only_US_YAY.csv'\n",
    "df_original = pd.read_csv(path+input_file, sep=';',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False, parse_dates=['folder_creation_date','date_last_change']) # set header=0 if i wanna pass it my own list of header names\n",
    "df_original = df_original.drop('Unnamed: 0', 1)\n",
    "df_original.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def most_common(lista):  # the list may have str and/or NANs\n",
    " \n",
    "    #print len(lista), lista\n",
    "  \n",
    "    new_lista = [str(x) for x in lista ]   # ojo!  str(np.nan)  = 'nan'\n",
    "    new_lista2= [ x for x in new_lista if x !=\"nan\" ]  # i remove the nans  \n",
    "   \n",
    "    #print new_lista2,len(new_lista2)\n",
    "    #raw_input()\n",
    "    if len(new_lista2)>0:\n",
    "        return max(set(new_lista2), key=new_lista2.count)   # i pick the most common (str) element\n",
    "    else:\n",
    "        return np.nan\n",
    "  \n",
    "    \n",
    "\n",
    " ##################################\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/\"\n",
    "    \n",
    "    filename=\"dict_lat_log_aggregate_activity___.pickle\"\n",
    "    file = open(path+filename,'r')\n",
    "    dict_lat_log_aggregate_activity = pickle.load(file)\n",
    "   \n",
    "    \n",
    "    filename=\"dict_tupla_location_df_index___.pickle\"\n",
    "    file = open(path+filename,'r')\n",
    "    dict_tupla_index = pickle.load(file)\n",
    "  \n",
    "    \n",
    "except:   \n",
    "\n",
    "    dict_tupla_index={}\n",
    "    dict_lat_log_aggregate_activity={}\n",
    "\n",
    "    \n",
    "   \n",
    "    for row in df_original.iterrows():\n",
    "\n",
    "        lat=row[1]['lat']\n",
    "        lon=row[1]['long']\n",
    "        key=(lat,lon)\n",
    "\n",
    "#         try:\n",
    "#             key=dict_tupla_index[tupla_lat_long]  # if i have already encounter that lat-long in the df\n",
    "#         except KeyError:\n",
    "#             dict_tupla_index[tupla_lat_long]=key\n",
    "#             key +=1\n",
    "          \n",
    "#         print \"keys:\",len(set(list(dict_tupla_index.keys()))), \"   values:\",len(set(list(dict_tupla_index.values()))),\n",
    "\n",
    "        num_adds= row[1]['num_adds']\n",
    "        num_edits= row[1]['num_edits']\n",
    "        num_deletes= row[1]['num_deletes']\n",
    "\n",
    "\n",
    "        num_publ = row[1]['group_total_publ']\n",
    "        num_publ_last = row[1]['group_num_papers_last']\n",
    "        num_cit = row[1]['group_num_citations']\n",
    "        \n",
    "#         if row[1]['Cleaned_university_name']==\"university of miami\":\n",
    "#             print row[0],\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            dict_lat_log_aggregate_activity[key]\n",
    "\n",
    "\n",
    "\n",
    "        except KeyError:  # if it is the first time i encounter that geolocation\n",
    "\n",
    "            dict_lat_log_aggregate_activity[key]={}  \n",
    "\n",
    "            dict_lat_log_aggregate_activity[key]['University']=row[1]['Cleaned_university_name']\n",
    "            dict_lat_log_aggregate_activity[key]['zip']=row[1]['zip']      \n",
    "            dict_lat_log_aggregate_activity[key]['lat']=row[1]['lat']      \n",
    "            dict_lat_log_aggregate_activity[key]['long']=row[1]['long']      \n",
    "            dict_lat_log_aggregate_activity[key]['Rural_Urban_Continuum_Code']=row[1]['Rural_Urban_Continuum_Code']   \n",
    "\n",
    "\n",
    "\n",
    "            dict_lat_log_aggregate_activity[key]['num_adds']=[]           \n",
    "            dict_lat_log_aggregate_activity[key]['num_edits']=[]           \n",
    "            dict_lat_log_aggregate_activity[key]['num_deletes']=[]\n",
    "           \n",
    "            \n",
    "            dict_lat_log_aggregate_activity[key]['common_num_publ']=[]           \n",
    "            dict_lat_log_aggregate_activity[key]['common_num_papers_last']=[]            \n",
    "            dict_lat_log_aggregate_activity[key]['common_num_citations']=[]\n",
    "            \n",
    "            \n",
    "            \n",
    "        dict_lat_log_aggregate_activity[key]['num_adds'].append(num_adds)                        \n",
    "        dict_lat_log_aggregate_activity[key]['num_edits'].append(num_edits)        \n",
    "        dict_lat_log_aggregate_activity[key]['num_deletes'].append(num_deletes)\n",
    "        dict_lat_log_aggregate_activity[key]['common_num_publ'].append(num_publ)\n",
    "        dict_lat_log_aggregate_activity[key]['common_num_papers_last'].append(num_publ_last)\n",
    "        dict_lat_log_aggregate_activity[key]['common_num_citations'].append(num_cit)\n",
    "\n",
    "            \n",
    "     \n",
    "\n",
    "    ############  i sum up all activity\n",
    "    for key in dict_lat_log_aggregate_activity:\n",
    "\n",
    "        dict_lat_log_aggregate_activity[key]['num_deletes']= np.nansum(dict_lat_log_aggregate_activity[key]['num_deletes'])  # sum ignoring nans\n",
    "        dict_lat_log_aggregate_activity[key]['num_adds']= np.nansum(dict_lat_log_aggregate_activity[key]['num_adds'])\n",
    "        dict_lat_log_aggregate_activity[key]['num_edits']= np.nansum(dict_lat_log_aggregate_activity[key]['num_edits'])\n",
    "\n",
    "\n",
    "        dict_lat_log_aggregate_activity[key]['tot_activity']=  dict_lat_log_aggregate_activity[key]['num_deletes'] + dict_lat_log_aggregate_activity[key]['num_adds'] + dict_lat_log_aggregate_activity[key]['num_edits']\n",
    "\n",
    "        try:\n",
    "            dict_lat_log_aggregate_activity[key]['log_tot_activity']=math.log(dict_lat_log_aggregate_activity[key]['tot_activity'])\n",
    "        except :\n",
    "\n",
    "            dict_lat_log_aggregate_activity[key]['log_tot_activity']=0.\n",
    "\n",
    "\n",
    "        dict_lat_log_aggregate_activity[key]['common_num_publ']= most_common(dict_lat_log_aggregate_activity[key]['common_num_publ'])  # these are list of strings and nans\n",
    "        dict_lat_log_aggregate_activity[key]['common_num_papers_last']= most_common(dict_lat_log_aggregate_activity[key]['common_num_papers_last'])\n",
    "        dict_lat_log_aggregate_activity[key]['common_num_citations']= most_common(dict_lat_log_aggregate_activity[key]['common_num_citations'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    #     if dict_lat_log_aggregate_activity[key]['University']=='university of miami':\n",
    "    #         print dict_lat_log_aggregate_activity[key]\n",
    "    #         print \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############# i dump the dict so i dont have to calculate them every time\n",
    "    path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/\"\n",
    "\n",
    "    filename=\"dict_lat_log_aggregate_activity.pickle\"\n",
    "    with open(path+filename,'wb') as f:\n",
    "        pickle.dump(dict_lat_log_aggregate_activity, f)\n",
    "    print \"written:\",path+filename\n",
    "\n",
    "\n",
    "    filename=\"dict_tupla_location_df_index.pickle\"\n",
    "    with open(path+filename,'wb') as f:\n",
    "        pickle.dump(dict_tupla_index, f)\n",
    "    print \"written:\",path+filename\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
