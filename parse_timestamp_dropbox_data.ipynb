{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt   \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "import scipy\n",
    "import operator\n",
    "import difflib\n",
    "import math\n",
    "\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # to make the notebook use the entire width of the browser\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle  ##################################\n",
    "   #it is faster than pickle!\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "import unicodedata\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import seaborn as sns   ### https://seaborn.pydata.org/tutorial/categorical.html\n",
    "import time  \n",
    "\n",
    "\n",
    "\n",
    "import plotly.plotly as py\n",
    "\n",
    "\n",
    "# i only need my credentials if i want to plot online --- and send plots to server (limits per day apply!)\n",
    "#import plotly.tools as tls\n",
    "#tls.set_credentials_file(username='juliettapc', api_key='deyNIvtOoDZ5PLmrHlhd')  # my plotly account credentials\n",
    "\n",
    "\n",
    "import pygraphviz\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## to be able to plot offline (without sending the plots to the plotly server every time)\n",
    "import plotly.offline as offline\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "################\n",
    "\n",
    "\n",
    "\n",
    "##### for getting geolocation data  and to calculate distance between two geolocations\n",
    "import requests\n",
    "import json\n",
    "import geopy.distance   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path = \"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "# input_file = 'Dropbox_datafile_complete_new_ranking.csv'\n",
    "# #input_file = 'Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_population_density_when_available_career_stage_GINI_folder_act_categories.csv'\n",
    "# df_old = pd.read_csv(path+input_file, sep=';',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False, parse_dates=['folder_creation_date','date_last_change']) # set header=0 if i wanna pass it my own list of header names\n",
    "# df_old.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# print df_old.shape   #  1403349, 38 \n",
    "\n",
    "\n",
    "\n",
    "# path = \"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "# input_file = 'ORIGINAL_Dropbox_datafile_may22_2017.csv'\n",
    "# #input_file = 'Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_population_density_when_available_career_stage_GINI_folder_act_categories.csv'\n",
    "# df_old_old = pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False)#, parse_dates=['folder_creation_date','date_last_change']) # set header=0 if i wanna pass it my own list of header names\n",
    "# print df_old_old.shape  \n",
    "\n",
    "\n",
    "# ######## load dictionary aggregated info by folder\n",
    "# pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_folder_id_folder_attr_even_more_var_.pickle'\n",
    "# with open(pickle_name, 'rb') as handle:\n",
    "#     dict_folder_id_folder_attr = pickle.load(handle)\n",
    " \n",
    "# print len(dict_folder_id_folder_attr.keys())\n",
    "# # df_folders = pd.DataFrame.from_dict(dict_folder_id_folder_attr,orient='index')\n",
    "# # print df_folders.shape   # 521274, 13\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######## load dictionary aggregated info by user\n",
    "# pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_user_id_user_attr_more_var_.pickle'\n",
    "# with open(pickle_name, 'rb') as handle:\n",
    "#     dict_user_id_user_attr = pickle.load(handle)\n",
    "# print len(dict_user_id_user_attr.keys())\n",
    "\n",
    "# # df_users = pd.DataFrame.from_dict(dict_user_id_user_attr,orient='index')\n",
    "# # #df_selection_users=df_folders[df_folders['user_univ_ranking']<=50]\n",
    "# # print df_users.shape   # 440353, 11\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ############  load the aggregated network\n",
    "# pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/Networks/network_all_new.pickle'\n",
    "# file = open(pickle_name,'r')\n",
    "# G_no_filter = pickle.load(file)\n",
    "# print \"\\n\",pickle_name, len (G_no_filter.nodes()), len(G_no_filter.edges())    #   440353 3659098\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ############  load the aggregated network   filtering links by activity (if a node's activity =0, then it is not link to any other members of a folder)\n",
    "# pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/Networks/network_all_weighted_by_act_thresh1.pickle'\n",
    "# file = open(pickle_name,'r')\n",
    "# G_activity = pickle.load(file)\n",
    "# print \"\\n\",pickle_name, len (G_activity.nodes()), len(G_activity.edges())    #   440353 3659098\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print df_old_old.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lista_users_old=list(df_old.user_id.unique())\n",
    "# lista_users_old_old=list(df_old_old.member_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df[df['user_id']==1842650] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_old[df_old['user_id']==1842650]#.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file.......\n"
     ]
    },
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-44af7bfbbb5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'new_DROPBOX_add_univ_names_country.csv'\u001b[0m    \u001b[0;31m#   'new_DROPBOX.csv'   #     new_sorted_no_duplicates.csv'   #ORIGINAL_Nico_Results_Hashed_RH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NAN\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"null\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'folder_creation_date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# usecols=[0,1,2,4,5,6,7,8,11,12,13,14,15], set header=0 if i wanna pass it my own list of header names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#df.drop('unnamed', axis=1, inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m _parser_defaults = {\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skip_footer not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:8771)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:9901)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas/parser.c:23325)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# header_names = ['folder_id','num_members','folder_creation_date','last_date_DO_NOT_TRUST','user_id','email_domain','num_adds','num_edits','num_dels',\\\n",
    "#                 'major_content_type','major_file_ext','date','field','new_group_total_pubs','new_group_num_papers_last','new_group_num_citations']\n",
    "    \n",
    "    \n",
    "# header_names = ['folder_id','num_members','folder_creation_date','last_date_DO_NOT_TRUST','user_id','email_domain','num_adds','num_edits','num_dels',\\\n",
    "#                 'major_content_type','major_file_ext','date','field','new_group_total_pubs','new_group_num_papers_last','new_group_num_citations',\\\n",
    "#                 'simplified_domain','University_Name','Cleaned_university_name','Country']\n",
    " \n",
    "header_names = ['unnamed', 'folder_id','num_members','folder_creation_date','user_id','num_adds','num_edits','num_dels','date','field',\\\n",
    "                'new_group_total_pubs','new_group_num_papers_last','new_group_num_citations','Cleaned_university_name','Country']\n",
    "\n",
    "\n",
    "\n",
    "print \"reading file.......\"\n",
    "\n",
    "path = \"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "input_file = 'new_DROPBOX_add_univ_names_country.csv'    #   'new_DROPBOX.csv'   #     new_sorted_no_duplicates.csv'   #ORIGINAL_Nico_Results_Hashed_RH\n",
    "\n",
    "df_user = pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False, usecols=[4,9,10,11,12,13,14],  parse_dates=['folder_creation_date','date']) # usecols=[0,1,2,4,5,6,7,8,11,12,13,14,15], set header=0 if i wanna pass it my own list of header names\n",
    "#df.drop('unnamed', axis=1, inplace=True)\n",
    "\n",
    "print df_user.shape   #  (9018863, 16) \n",
    "df_user=df.drop_duplicates()  # i have dropped them from the shell already\n",
    "print df_user.shape , \"after dropping duplicates\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_name=path+\"df_new_dropbox_users.csv\"\n",
    "df_user.to_csv(new_name, sep=',')\n",
    "print \"written:\", new_name,\"\\n\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(df.folder_id.unique()) , \"unique folders\"#  345,043 users,     417,657  folders\n",
    "print len(df.user_id.unique()) , \"unique users\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_complete_info=df[['folder_id','user_id','email','new_group_num_citations']].dropna(how='any')  # by default, dropna drops a row if ANY  of the indicated fields are missing (i can also set it to if ALL the fields are missing: how='all')\n",
    "print df_complete_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.folder_creation_date.max()    # min date: 2015-05-15 , max: 2017-05-15       ## folder creation date, min: 2008-05-01,  max: 2017-05-16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['user_id'] ==881645].sort_values(by=['folder_id','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#to get rid of the cumulative countings and redundant rows without new info, and get actual new activity counts for each date\n",
    "\n",
    "def  grouping(input_df):\n",
    "\n",
    "   \n",
    "    new_var=np.nan \n",
    "    user_id =  input_df.user_id.iloc[0]\n",
    "    folder_id= input_df.folder_id.iloc[0]\n",
    "    new_var= str(user_id) +\"_\" +str(folder_id)\n",
    "    \n",
    "    \n",
    "    \n",
    "    list_values_add=[]   # i need to create a list of actual activity values, not cumulative ones\n",
    "    list_values_edit=[]\n",
    "    list_values_del=[]    \n",
    "        \n",
    "   \n",
    "        \n",
    "                \n",
    "    list_indeces=[]\n",
    "    flag_new=1\n",
    "\n",
    "    old_tupla_activity=(None,None,None)        \n",
    "\n",
    "    old_value_add=0\n",
    "    old_value_edit=0\n",
    "    old_value_del=0\n",
    "    old_date=pd.Timestamp('1982-01-01')  # arbitrarily old date\n",
    "      \n",
    "    \n",
    "    cont=1\n",
    "    for row in input_df.iterrows():\n",
    "        index=row[0]\n",
    "\n",
    "        num_adds=row[1].num_adds_last28days\n",
    "        num_edits=row[1].num_edits_last28days\n",
    "        num_dels=row[1].num_dels_last28days\n",
    "\n",
    "        tupla_activity=(num_adds,num_edits ,num_dels)  #  num_adds_last28days','num_edits_last28days','num_dels_last28days\n",
    "\n",
    "\n",
    "        date=row[1].date_act\n",
    "        try:\n",
    "            print old_date, date, (date-old_date).days\n",
    "            raw_input()\n",
    "        except:\n",
    "            print date, \"no date, no problem\"\n",
    "        \n",
    "        if tupla_activity == old_tupla_activity:\n",
    "            flag_new =0\n",
    "        else:\n",
    "            flag_new=1\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        if flag_new ==1:   # OJO!!!  double check! THE RESETTING TO TAKE INTO ACCOUNT THE 28 DAY PERIOD!!!!!!!!11  \n",
    "\n",
    "\n",
    "            list_indeces.append(index)\n",
    "\n",
    "\n",
    "            if old_value_add  <=   num_adds: \n",
    "                actual_val_add = num_adds - old_value_add\n",
    "            else:\n",
    "                actual_val_add = num_adds                                                                         \n",
    "\n",
    "            if old_value_edit <= num_edits:       \n",
    "                actual_val_edits = num_edits- old_value_edit\n",
    "            else:\n",
    "                actual_val_edits = num_edits\n",
    "\n",
    "            if old_value_del  <=  num_dels:\n",
    "                actual_val_del = num_dels - old_value_del                \n",
    "            else:\n",
    "                actual_val_del = num_dels \n",
    "\n",
    "\n",
    "\n",
    "            list_values_add.append(actual_val_add)\n",
    "            list_values_edit.append(actual_val_edits)\n",
    "            list_values_del.append(actual_val_del)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        old_tupla_activity=tupla_activity\n",
    "\n",
    "\n",
    "        old_value_add= num_adds\n",
    "        old_value_edit= num_edits\n",
    "        old_value_del= num_dels\n",
    "\n",
    "\n",
    "        cont +=1\n",
    "\n",
    "        selected_df=input_df.ix[list_indeces]\n",
    "\n",
    "\n",
    "        selected_df['num_adds'] = pd.Series(list_values_add, index=list_indeces)\n",
    "        selected_df['num_edits'] = pd.Series(list_values_edit, index=list_indeces)\n",
    "        selected_df['num_dels'] = pd.Series(list_values_del, index=list_indeces)\n",
    "    \n",
    " \n",
    "\n",
    "    return selected_df\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    " \n",
    "##########################\n",
    "\n",
    "    \n",
    "df_filtered = pd.DataFrame(columns=header_names)\n",
    "cont_tot=0\n",
    "cont_selection=0\n",
    "for user_id in tqdm_notebook(df['user_id'].head(50000).unique()):\n",
    "        \n",
    "    df_selection_user= df[df['user_id'] == user_id]\n",
    "    \n",
    "    for folder_id in df_selection_user['folder_id'].unique():\n",
    "        \n",
    "        df_sub_selection =  df_selection_user[df_selection_user['folder_id'] == folder_id].sort_values(by='date')\n",
    "                                          \n",
    "        cont_tot += len(df_sub_selection)\n",
    "        \n",
    "   \n",
    "\n",
    "        df_result= grouping(df_sub_selection)\n",
    "        df_filtered = pd.concat([df_filtered,df_result])\n",
    "        \n",
    "        cont_selection += len(df_result)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "print \"done\"  # 667133998     18092141 (many rows!)    298373200    87241372    205282877   205308895\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print  df.shape, df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "input_file = 'new_sorted_no_duplicates.csv'\n",
    "new_file_name=path+input_file.replace(\".csv\",\"\")+\"_processed_timestaps_PARTIAL.csv\"\n",
    "df_filtered.to_csv(new_file_name, sep=',')\n",
    "print \"written:\", new_file_name,\"\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cont_tot, cont_selection   #one row: 130515361    3327476   108763580\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['user_id']==34049545].sort_values(by='date')  # 273831436    383267705  667133998  18092141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_filtered[df_filtered['user_id']==34049545].sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "18092141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['user_id']==273831436].sort_values(by='date')  #.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_new[df_new['user_id']==96751].sort_values(by='date')\n",
    "#df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lista_users=list(df.user_id.unique())\n",
    "# set(lista_users)  & set(lista_users_old_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  df=pandas.read_csv(path+filename, sep=',',names = ['First_Name','Last_Name','Title','Email','University'-,'School_college','Departments','Url'])\n",
    "    \n",
    "#     usecols=[\"date\", \"loc\", \"x\"]\n",
    "    \n",
    "# #     df.rename(columns={'old_col':'new_col', 'old_col_2':'new_col_2'}, inplace=True)\n",
    "    \n",
    "# #     df.rename(columns={'old_col':'folder_id', 'old_col':'num_members', 'old_col':'folder_creation_date', 'old_col':'last_date_DO_NOT_TRUST', 'old_col':'user_id',\\\n",
    "# #                        'old_col':'email', 'old_col':'num_adds_last28days', 'old_col':'num_edits_last28days', 'old_col_2':'num_dels_last28days', \\\n",
    "# #                        'old_col_2':'new_col_2', 'old_col_2':'new_col_2', 'old_col_2':'new_col_2', 'old_col_2':'new_col_2', 'old_col_2':'new_col_2', 'old_col_2':'new_col_2', 'old_col_2':'new_col_2', 'old_col_2':'new_col_2', 'old_col_2':'new_col_2'}, inplace=True)\n",
    "     \n",
    "    \n",
    "# names = ['folder_id','num_members','folder_creation_date','last_date_DO_NOT_TRUST','user_id','email','num_adds_last28days','num_edits_last28days','num_dels_last28days',\\\n",
    "#     'major_content_type','major_file_ext','date','field','new_group_total_pubs','new_group_num_papers_last','new_group_num_citations']\n",
    "    \n",
    "    \n",
    "    \n",
    "#     If you only want to read the first 999,999 (non-header) rows:\n",
    "\n",
    "# read_csv(..., nrows=999999)\n",
    "# If you only want to read rows 1,000,000 ... 1,999,999\n",
    "\n",
    "# read_csv(..., skiprows=1000000, nrows=999999)\n",
    "\n",
    "# FALTA DROP DUPLICATES!   df=df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
