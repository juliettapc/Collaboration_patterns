{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt   \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "import scipy\n",
    "import operator\n",
    "import difflib\n",
    "import math\n",
    "from IPython.core.display import display,HTML\n",
    "try:\n",
    "    import cPickle as pickle     #it is faster than pickle!\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "import unicodedata\n",
    "import networkx as nx\n",
    "import itertools\n",
    "#import seaborn as sns   ### https://seaborn.pydata.org/tutorial/categorical.html\n",
    "    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # to make the notebook use the entire width of the browser\n",
    "\n",
    "\n",
    "\n",
    "# import plotly.plotly as py\n",
    "# from plotly.graph_objs import *\n",
    "\n",
    "# # i only need my credentials if i want to plot online --- and send plots to server (limits per day apply!)\n",
    "# #import plotly.tools as tls\n",
    "# #tls.set_credentials_file(username='juliettapc', api_key='deyNIvtOoDZ5PLmrHlhd')  # my plotly account credentials\n",
    "\n",
    "\n",
    "# import pygraphviz\n",
    "# from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########## to be able to plot offline (without sending the plots to the plotly server every time)\n",
    "# import plotly.offline as offline\n",
    "# from plotly.graph_objs import *\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "# init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6527213, 7)\n",
      "after removing rows without dates:\n",
      "  num. users: 175587 (5599544, 7)\n",
      "438284\n",
      "(438284, 24)\n",
      "read: /home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_folder_id_list_users_COMPLETE.pickle 519045\n",
      "read: /home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_user_id_list_folders_COMPLETE.pickle 438284\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "input_file = 'DROPBOX_only_activity_aggr_by_date_COMPLETE.csv'  # one line per user, folder, and date of activity\n",
    "\n",
    "df = pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False, parse_dates=['folder_creation_date','date']) # set header=0 if i wanna pass it my own list of header names\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "print df.shape\n",
    "\n",
    "\n",
    "\n",
    "df=df.replace('nan', np.nan)  ## for some reason, there are diff nomenclatures for NANs, and it is not interpreting them correctly when first reading the file \n",
    "df=df.replace('NAN', np.nan)\n",
    "df=df.replace('NaN', np.nan)\n",
    "df=df.replace('-1', np.nan)\n",
    "df=df.replace('', np.nan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#convert the date columns from str to datetime    (for some reason, parsing when i read it does nothing)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['folder_creation_date'] = pd.to_datetime(df['folder_creation_date'], errors='coerce')\n",
    "\n",
    "\n",
    "df= df[df.date.notnull()]    \n",
    "print \"after removing rows without dates:\\n  num. users:\",len(df.user_id.unique()),df.shape    # 400000  (5141758, 7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## load dictionary aggregated info by user                                \n",
    "pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_user_id_user_attr_new_COMPLETE.pickle'\n",
    "with open(pickle_name, 'rb') as handle:\n",
    "    dict_user_id_user_attr = pickle.load(handle)\n",
    "print len(dict_user_id_user_attr.keys())\n",
    "\n",
    "df_users = pd.DataFrame.from_dict(dict_user_id_user_attr,orient='index')\n",
    "df_users['user_id'] = df_users.index\n",
    "print df_users.shape   # 440353, 11\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_folder_id_list_users_COMPLETE.pickle'\n",
    "with open(pickle_name, 'rb') as handle:\n",
    "        dict_folder_list_users = pickle.load(handle)\n",
    "print \"read:\", pickle_name, len(dict_folder_list_users)\n",
    "       \n",
    "pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_user_id_list_folders_COMPLETE.pickle'\n",
    "with open(pickle_name, 'rb') as handle:\n",
    "        dict_user_list_folders = pickle.load(handle)\n",
    "print \"read:\",pickle_name,  len(dict_user_list_folders)\n",
    "    \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_users.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# dict_user_id_user_attr   \n",
    "# # 629145600.0: {'Country': 'United States',\n",
    "# #   'University_name': 'cornell university',\n",
    "# #   'career_stage': nan,\n",
    "# #   'category_total_cit': nan,\n",
    "# #   'category_total_last_auth': nan,\n",
    "# #   'category_total_publ': nan,\n",
    "# #   'eff_num_folders': 0,\n",
    "# #   'email_domain': 'cornell.edu',\n",
    "# #   'field': nan,\n",
    "# #   'folders_above_avg': {1369112819.0: 0},\n",
    "# #   'geoloc': '(42.4534492, -76.4735027) 14850',\n",
    "# #   'gini_act_across_folders': nan,\n",
    "# #   'group_num_citations': nan,\n",
    "# #   'group_num_papers_last': nan,\n",
    "# #   'group_total_publ': nan,\n",
    "# #   'national_ranking': 8.0,\n",
    "# #   'number_folders': 1,\n",
    "# #   'user_id': 629145600.0,\n",
    "# #   'user_tot_act': 0.0,\n",
    "# #   'user_tot_num_adds': 0.0,\n",
    "# #   'user_tot_num_deletes': 0.0,\n",
    "# #   'user_tot_num_edits': 0.0,\n",
    "# #   'user_univ_ranking': 10.0,\n",
    "# #   'world_ranking': 10.0},\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #### time window for temporal networks\n",
    "# string_start_date='2014-05-01'\n",
    "# start_date=pd.Timestamp(string_start_date)\n",
    "\n",
    "# #string_end_date='2014-11-01'\n",
    "# end_date=start_date + pd.Timedelta('180 days')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list_tupla_dates=[]\n",
    "# for i in range(6):\n",
    "#     print start_date, end_date    \n",
    "#     tupla_dates=[start_date,end_date]\n",
    "#     list_tupla_dates.append(tupla_dates)\n",
    "\n",
    "    \n",
    "#     start_date += pd.Timedelta('180 days')\n",
    "#     end_date += pd.Timedelta('180 days')\n",
    "\n",
    "# # 2014-05-01_2014-10-28\n",
    "\n",
    "\n",
    "\n",
    "### for plotting time series directly from data\n",
    "#df.plot(x='folder_creation_date',y='folder_lifespan',ls='',marker='.') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_nans(lista):\n",
    "   # print \"\\n\",lista\n",
    "    aux_lista=[]\n",
    "    for i in range(len(lista)):    \n",
    "       # print lista[i]\n",
    "        try:\n",
    "            if np.isnan(lista[i]) == True:  # (when nan):   # OJO!!! NO SIRVE SI HAGO: if lista[I]== np.nan   :(   !!!\n",
    "                pass#print \"nan found\"\n",
    "            else: # (when not-nan):           \n",
    "                aux_lista.append(lista[i])\n",
    "            #print \"no problem\"\n",
    "        except : ## whenever it is a STR            \n",
    "            pass            \n",
    "            \n",
    "            \n",
    "    #print aux_lista   \n",
    "    return aux_lista\n",
    "\n",
    "\n",
    "############################3\n",
    "\n",
    "\n",
    "\n",
    "def remove_nans_for_strings(lista):\n",
    "   # print \"\\n\",lista\n",
    "    aux_lista=[]\n",
    "    for i in range(len(lista)):    \n",
    "       # print lista[i]\n",
    "        try:\n",
    "            if np.isnan(lista[i]) == True:  # (when nan):   # OJO!!! NO SIRVE SI HAGO: if lista[I]== np.nan   :(   !!!\n",
    "                pass#print \"nan found\"\n",
    "            else: # (when not-nan):           \n",
    "                aux_lista.append(lista[i])\n",
    "            #print \"no problem\"\n",
    "        except : ## whenever it is a STR            \n",
    "             aux_lista.append(lista[i])#pass             \n",
    "            \n",
    "            \n",
    "    #print aux_lista   \n",
    "    return aux_lista\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "def remove_nans_replace_by_zeros(lista):\n",
    "   # print \"\\n\",lista\n",
    "    aux_lista=[]\n",
    "    for i in range(len(lista)):    \n",
    "       # print lista[i]\n",
    "        try:\n",
    "            if np.isnan(lista[i]) == True:  # (when nan):   # OJO!!! NO SIRVE SI HAGO: if lista[I]== np.nan   :(   !!!\n",
    "                aux_lista.append(0.)        #pass#print \"nan found\"\n",
    "            else: # (when not-nan):           \n",
    "                aux_lista.append(float(lista[i]))\n",
    "            #print \"no problem\"\n",
    "        except : ## whenever it is a STR            \n",
    "            pass             \n",
    "            \n",
    "            \n",
    "    #print aux_lista   \n",
    "    return aux_lista\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### get dict with  folder_id lifespan:\n",
    "def remove_nans_replace_intervals_with_starting_value(lista):\n",
    "    #print \"\\n\",lista\n",
    "    aux_lista=[]\n",
    "    for i in range(len(lista)):    \n",
    "       # print lista[i]\n",
    "        try:\n",
    "            if np.isnan(lista[i]) == True:  # (when/ nan):   # OJO!!! NO SIRVE SI HAGO: if lista[I]== np.nan   :(   !!!\n",
    "                pass#print \"nan found\"\n",
    "        except TypeError: # (when not-nan):\n",
    "            #print lista[i]   \n",
    "            try:  \n",
    "                aux_lista.append(float(lista[i]))\n",
    "                #print \"no problem\"\n",
    "            except ValueError:  # sometimes, the ranking is 75-82\n",
    "                #print \"issue found\"\n",
    "                value=lista[i]\n",
    "#                 v1=float(value.split(\"-\")[0])\n",
    "#                 v2=float(value.split(\"-\")[1])\n",
    "                \n",
    "                new_value=float(value.split(\"-\")[0])#np.median([v1,v2])  \n",
    "                aux_lista.append(new_value)\n",
    "            \n",
    "    #print aux_lista   \n",
    "    return aux_lista\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "\n",
    "\n",
    "def gini(old_list_of_values):\n",
    "    \n",
    "    list_of_values=[]\n",
    "    for item in old_list_of_values:\n",
    "        try:\n",
    "            int(item) # if it is a NAN, it with fail\n",
    "            list_of_values.append(item)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "   # print list_of_values\n",
    "    sorted_list = sorted(list_of_values)\n",
    "    height, area = 0, 0\n",
    "    for value in sorted_list:\n",
    "        height += value\n",
    "        area += height - value / 2.\n",
    "    fair_area = height * len(list_of_values) / 2.\n",
    "    try:\n",
    "        return (fair_area - area) / fair_area\n",
    "    except ZeroDivisionError:\n",
    "#         print \"problems with:\",list_of_values   # if lista=[0,0,0]  or [0]\n",
    "#         raw_input()\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "\n",
    "def  effective_num(lista_values):  # i can use it for the effective number of members in a folder, by activity, or the effective number of folders for a user, also by activity\n",
    "    \n",
    "     # first, i need to remove the zeros, because i cant do the log(0), but i can remove them, they dont count as effective members\n",
    "    \n",
    "    \n",
    "    cont_num_nonzero_items=0\n",
    "    H=0.\n",
    "    tot_sum=float(sum(lista_values))\n",
    "    for item in lista_values:\n",
    "        if item >0:\n",
    "            aux= item/tot_sum * np.log2( item/tot_sum)\n",
    "            H += aux\n",
    "            cont_num_nonzero_items +=1\n",
    "            \n",
    "    H = -1.0 * H\n",
    "    \n",
    "    eff_number = np.power(2.0, H)\n",
    "    \n",
    "    if cont_num_nonzero_items ==0:   # if the list of act is [0] then i want the eff. number to be 0, not 1\n",
    "        eff_number =0\n",
    "    \n",
    "    return eff_number\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0b193e5c6d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m        \u001b[0mdf_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'folder_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfolder_id\u001b[0m \u001b[0;34m)\u001b[0m  \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0muser_id\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    824\u001b[0m                       else fill_bool)\n\u001b[1;32m    825\u001b[0m             return filler(self._constructor(na_op(self.values, other.values),\n\u001b[0;32m--> 826\u001b[0;31m                                             index=self.index, name=name))\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/core/ops.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0mfill_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0mfill_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, raise_on_error, **kwargs)\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2949\u001b[0m         mgr = self._data.astype(dtype=dtype, copy=copy,\n\u001b[0;32m-> 2950\u001b[0;31m                                 raise_on_error=raise_on_error, **kwargs)\n\u001b[0m\u001b[1;32m   2951\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, raw, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2890\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2891\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, raise_on_error, values, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m                **kwargs):\n\u001b[1;32m    433\u001b[0m         return self._astype(dtype, copy=copy, raise_on_error=raise_on_error,\n\u001b[0;32m--> 434\u001b[0;31m                             values=values, **kwargs)\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     def _astype(self, dtype, copy=False, raise_on_error=True, values=None,\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_astype\u001b[0;34m(self, dtype, copy, raise_on_error, values, klass, mgr, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/juliaponcela/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep, mgr)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "     \n",
    "        \n",
    "dict_folder_dict_user_act_in_folder={}\n",
    "\n",
    "for folder_id in tqdm_notebook(dict_folder_list_users):  \n",
    "\n",
    "        list_users_in_folder=dict_folder_list_users[folder_id]\n",
    "        \n",
    "        for user_id in list_users_in_folder:\n",
    "            \n",
    "       \n",
    "            df_select = df[(df['folder_id'] == folder_id )  & (df['user_id']==user_id )]\n",
    "    \n",
    "           \n",
    "        \n",
    "            num_adds=sum(remove_nans_replace_by_zeros(list(df_select.num_adds))) # for activity records, NAN = 0\n",
    "            num_edits=sum(remove_nans_replace_by_zeros(list(df_select.num_edits)))\n",
    "            num_deletes=sum(remove_nans_replace_by_zeros(list(df_select.num_dels)))\n",
    "            user_act=num_adds+num_edits+num_deletes\n",
    "      \n",
    "\n",
    "        try: \n",
    "            dict_folder_dict_user_act_in_folder[folder_id]\n",
    "        except KeyError:\n",
    "            dict_folder_dict_user_act_in_folder[folder_id]={}            \n",
    "\n",
    "        dict_folder_dict_user_act_in_folder[folder_id][user_id]=user_act\n",
    "\n",
    "        \n",
    "\n",
    "print \"done with the dicts., size:\",len(dict_folder_list_users),len(dict_folder_dict_user_act_in_folder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/\"\n",
    "filename=\"dict_folder_dict_user_act_in_folder_PARTIAL.pickle\"\n",
    "with open(path+filename,'wb') as f:\n",
    "    pickle.dump(dict_folder_dict_user_act_in_folder, f)\n",
    "print \"written:\",path+filename\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# ######## load dictionary of dict for users' act                            \n",
    "# pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/new_dict_folder_dict_user_act_in_folder_more_attr_COMPLETE.pickle'\n",
    "# with open(pickle_name, 'rb') as handle:\n",
    "#     dict_folder_dict_user_act_in_folder_more_attr = pickle.load(handle)\n",
    "# print len(dict_folder_dict_user_act_in_folder_more_attr.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/new_dict_folder_dict_user_act_in_folder.pickle'\n",
    "# with open(pickle_name, 'rb') as handle:\n",
    "#     dict_folder_dict_user_act_in_folder = pickle.load(handle)\n",
    "# print len(dict_folder_dict_user_act_in_folder.keys())\n",
    "\n",
    "dict_folder_dict_user_act_in_folder=dict_folder_dict_user_act_in_folder_more_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dict_user_id_user_attr\n",
    "# 1053958.0: {'Country': 'United States',\n",
    "#   'University_name': 'university of washington',\n",
    "#   'career_stage': nan,\n",
    "#   'category_total_cit': nan,\n",
    "#   'category_total_last_auth': nan,\n",
    "#   'category_total_publ': nan,\n",
    "#   'eff_num_folders': 1.5069930657725379,\n",
    "#   'field': 'science',\n",
    "#   'gini_act_across_folders': 0.8571428571428571,\n",
    "#   'number_folders': 9,\n",
    "#   'user_tot_act': 7.0,\n",
    "#   'user_tot_num_adds': 4.0,\n",
    "#   'user_tot_num_deletes': 3.0,\n",
    "#   'user_tot_num_edits': 0.0,\n",
    "#   'user_univ_ranking': 31.0},\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "######## unweighted network:\n",
    "list_missing_nodes=[]\n",
    "G=nx.Graph()\n",
    "for folder_id in tqdm_notebook(dict_folder_list_users):\n",
    "        #dict_folder_list_users[folder]=list(set(dict_folder_list_users[folder]))  # remove possible duplicates       (there shouldnt be any)\n",
    "\n",
    "        lista=dict_folder_list_users[folder_id]\n",
    "\n",
    "        if len(lista)>1:\n",
    "\n",
    "            lista_pares=itertools.combinations(lista, 2)        \n",
    "\n",
    "            for item in lista_pares:\n",
    "                e1=item[0]\n",
    "                e2=item[1]\n",
    "                G.add_edge(e1,e2)\n",
    "                \n",
    "                try: \n",
    "                    G.edge[e1][e2][\"num_common_projects\"] +=1\n",
    "                except KeyError:\n",
    "                    G.edge[e1][e2][\"num_common_projects\"] =1\n",
    "\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            n=lista[0]\n",
    "            G.add_node(n)\n",
    "            try:  # because of the partial dict of user's attributes \n",
    "                G.node[n][\"tot_act\"]=dict_user_id_user_attr[n]['user_tot_act']\n",
    "            except KeyError:\n",
    "                list_missing_nodes.append(n)\n",
    "                        \n",
    "\n",
    "print \"  N:\", len(G.nodes()),\"  L:\", len(G.edges())       \n",
    "GC = max(nx.connected_component_subgraphs(G), key=len)\n",
    "print \"\\n  GC:     N:\", len(GC.nodes()), \"  L:\", len(GC.edges())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/Networks/\"\n",
    "filename=\"network_all_new_PARTIAL.pickle\"\n",
    "with open(path+filename,'wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "print \"written:\",path+filename\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lista=[]\n",
    "for folder_id in dict_folder_list_users:\n",
    "    for user in dict_folder_list_users[folder_id]:\n",
    "        lista.append(user)\n",
    "        \n",
    "print len(set(lista)), len(dict_user_id_user_attr), len(list_missing_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(dict_folder_list_users), len(dict_folder_list_users), len(list_missing_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print df_users.user_tot_act.mean(), df_users.user_tot_act.quantile(q=0.6)\n",
    "act_threshold=1 #  28 is the 60% ,  56 is the 65%  ,   168 is the 70% percentile (50% percentile is 0 and 55% is also 0)\n",
    "\n",
    "print \"thershold for activity\",act_threshold\n",
    "\n",
    "list_missing_nodes=[]\n",
    "\n",
    "\n",
    "\n",
    "####### weighted network:\n",
    "\n",
    "G_weighted=nx.Graph()\n",
    "for folder_id in tqdm_notebook(dict_folder_list_users):\n",
    "       \n",
    "                    \n",
    "        lista_users=dict_folder_list_users[folder_id]\n",
    "       \n",
    "        weighted_list_users=[]\n",
    "        for user_id in lista_users:\n",
    "            \n",
    "            try:\n",
    "                user_act_in_folder=dict_folder_dict_user_act_in_folder[folder_id][user_id]\n",
    "                if user_act_in_folder >= act_threshold:\n",
    "                    weighted_list_users.append(user_id)\n",
    "            except KeyError:\n",
    "                list_missing_nodes.append(user_id)\n",
    "                pass\n",
    "                \n",
    "      \n",
    "        \n",
    "        if len(weighted_list_users)>1:  # ignore nodes that havent done any work in a particular folder (they will only count as isolated)\n",
    "\n",
    "            lista_pares=itertools.combinations(weighted_list_users, 2)        \n",
    "\n",
    "            for item in lista_pares:\n",
    "                e1=item[0]\n",
    "                e2=item[1]\n",
    "                G_weighted.add_edge(e1,e2)               \n",
    "                \n",
    "                try: \n",
    "                    G_weighted.edge[e1][e2][\"num_common_projects\"] +=1\n",
    "                except KeyError:\n",
    "                    G_weighted.edge[e1][e2][\"num_common_projects\"] =1\n",
    "                    \n",
    "\n",
    "                    \n",
    "        \n",
    "        for n in lista_users:\n",
    "            G_weighted.add_node(n)\n",
    "            try:  # because it the partial  dict for users' attr there are users missing\n",
    "                G_weighted.node[n][\"tot_act\"]=dict_user_id_user_attr[n]['user_tot_act']\n",
    "            except KeyError:\n",
    "                pass\n",
    "           \n",
    "\n",
    "            \n",
    "           \n",
    "        \n",
    "\n",
    "print \"  N:\", len(G_weighted.nodes()),\"  L:\", len(G_weighted.edges())       \n",
    "GC = max(nx.connected_component_subgraphs(G_weighted), key=len)\n",
    "print \"\\n  GC:     N:\", len(GC.nodes()), \"  L:\", len(GC.edges())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/Networks/\"\n",
    "filename=\"network_all_weighted_by_act_thresh\"+str(act_threshold)+\"_PARTIAL.pickle\"\n",
    "with open(path+filename,'wb') as f:\n",
    "    pickle.dump(G_weighted, f)\n",
    "print \"written:\",path+filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(list_missing_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### the master dict is larger than G.nodes(), and the set_node_attribute does not work in that case, so i get a selection of the dict only with the common nodes:\n",
    "#######################\n",
    "\n",
    "# new_dict_user_id = {k: dict_user_id_domain[k] for k in set(G.nodes()) & set(dict_user_id_domain.keys())}\n",
    "# print len(G.nodes()), len(new_dict_user_id)\n",
    "# # i add the email as node attributes    \n",
    "# nx.set_node_attributes(G, 'email', new_dict_user_id)\n",
    "# ############\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "9e5c72b386084605b43c67db72a92ca8": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
