{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt   \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from time import sleep\n",
    "import scipy\n",
    "import operator\n",
    "import difflib\n",
    "import math\n",
    "\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # to make the notebook use the entire width of the browser\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle  ##################################\n",
    "   #it is faster than pickle!\n",
    "except:\n",
    "    import pickle\n",
    "    \n",
    "import unicodedata\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import seaborn as sns   ### https://seaborn.pydata.org/tutorial/categorical.html\n",
    "import time  \n",
    "\n",
    "\n",
    "\n",
    "import plotly.plotly as py\n",
    "\n",
    "\n",
    "# i only need my credentials if i want to plot online --- and send plots to server (limits per day apply!)\n",
    "#import plotly.tools as tls\n",
    "#tls.set_credentials_file(username='juliettapc', api_key='deyNIvtOoDZ5PLmrHlhd')  # my plotly account credentials\n",
    "\n",
    "\n",
    "import pygraphviz\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## to be able to plot offline (without sending the plots to the plotly server every time)\n",
    "import plotly.offline as offline\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "################\n",
    "\n",
    "\n",
    "\n",
    "##### for getting geolocation data  and to calculate distance between two geolocations\n",
    "import requests\n",
    "import json\n",
    "import geopy.distance   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Urban-Rural codes:\n",
    "\n",
    "# Metropolitan Counties*\t\n",
    "# 1\tIn large metro area of 1+ million residents\n",
    "# 2\tIn small metro area of less than 1 million residents\n",
    "\n",
    "# Nonmetropolitan Counties\t\n",
    "# 3\tMicropolitan area adjacent to large metro area\n",
    "# 4\tNoncore adjacent to large metro area\n",
    "# 5\tMicropolitan area adjacent to small metro area\n",
    "# 6\tNoncore adjacent to small metro area and contains a town of at least 2,500 residents\n",
    "# 7\tNoncore adjacent to small metro area and does not contain a town of at least 2,500 residents\n",
    "# 8\tMicropolitan area not adjacent to a metro area\n",
    "# 9\tNoncore adjacent to micro area and contains a town of at least 2,500 residents\n",
    "# 10\tNoncore adjacent to micro area and does not contain a town of at least 2,500 residents\n",
    "# 11\tNoncore not adjacent to metro or micro area and contains a town of at least 2,500 residents\n",
    "# 12\tNoncore not adjacent to metro or micro area and does not contain a town of at least 2,500 residents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "# input_file='Dropbox_datafile_may22_2017_modified.csv'\n",
    "# df_original=pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False, parse_dates=['folder_creation_date','date_last_change']) # set header=0 if i wanna pass it my own list of header names\n",
    "# df_original = df_original.drop('Unnamed: 0', 1)\n",
    "# print df_original.shape\n",
    "# df_original=df_original.drop_duplicates(subset=[\"folder_id\",\"user_id\"])  # i drop any duplicates by those two indeces  OJO!! it drops from 2592500 rows to 1403328  \n",
    "# print df_original.shape\n",
    "\n",
    "# print len(df_original.email_domain.unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "# # input_file='Dropbox_datafile_may22_2017_modified.csv'\n",
    "# # df_original=pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False, parse_dates=['folder_creation_date','date_last_change']) # set header=0 if i wanna pass it my own list of header names\n",
    "# # df_original = df_original.drop('Unnamed: 0', 1)\n",
    "# # print df_original.shape\n",
    "# # df_original=df_original.drop_duplicates(subset=[\"folder_id\",\"user_id\"])  # i drop any duplicates by those two indeces  OJO!! it drops from 2592500 rows to 1403328  \n",
    "# # print df_original.shape\n",
    "\n",
    "# # print len(df_original.email_domain.unique())\n",
    "\n",
    "\n",
    "\n",
    "# header_names = ['folder_id','num_members','folder_creation_date','last_date_DO_NOT_TRUST','user_id','email_domain','num_adds','num_edits','num_dels',\\\n",
    "#                 'major_content_type','major_file_ext','date','field','new_group_total_pubs','new_group_num_papers_last','new_group_num_citations']\n",
    "\n",
    "# path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "\n",
    "# input_file='duplicates_removed_no_header.csv' \n",
    "\n",
    "# df = pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False,header=None,  names=header_names,  parse_dates=['folder_creation_date','date'], \\\n",
    "#                  usecols=[0,2,4,6,7,8])             #, set header=0 if i wanna pass it my own list of header names\n",
    "                 \n",
    "# #df.drop('last_date_DO_NOT_TRUST', axis=1, inplace=True)\n",
    "\n",
    "# print df.shape   # (9018863, 15)\n",
    "\n",
    "\n",
    "# input_file='duplicates_removed_no_header.csv' \n",
    "# df_user = pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False,header=None,  names=header_names,\\\n",
    "#                  usecols=[4,5,12,13,14,15], nrows=1000000)      #    parse_dates=['folder_creation_date','date'],       #, set header=0 if i wanna pass it my own list of header names\n",
    "                 \n",
    "# #df.drop('last_date_DO_NOT_TRUST', axis=1, inplace=True)\n",
    "\n",
    "# print df_user.shape   # (9018863, 15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #df_merged = df_merged.drop('simplified_domain_x', 1)\n",
    "# #df_merged = df_merged.drop('simplified_domain_y', 1)\n",
    "# #df_merged.rename(columns={'email_domain_x': 'email_domain'}, inplace=True)\n",
    "# df_user=df_user.drop_duplicates(subset=[\"user_id\"]) \n",
    "# print df_user.shape, \"  after dropping duplicate user_ids\"   # (9018863, 15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num. users: 438284\n",
      "num. folders: 519045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ######## load dictionaries for user: list_folders, and folder: list_users\n",
    "pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_user_id_list_folders_COMPLETE.pickle'\n",
    "with open(pickle_name, 'rb') as handle:\n",
    "    dict_user_id_list_folders = pickle.load(handle)\n",
    "print \"num. users:\",len(dict_user_id_list_folders)\n",
    "\n",
    "\n",
    "\n",
    "pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_folder_id_list_users_COMPLETE.pickle'\n",
    "with open(pickle_name, 'rb') as handle:\n",
    "    dict_folder_id_list_users = pickle.load(handle)\n",
    "print \"num. folders:\",len(dict_folder_id_list_users)\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_all_users=dict_user_id_list_folders.keys()\n",
    "\n",
    "list_section_users=list_all_users[:10000]\n",
    "len(list_section_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juliaponcela/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:18: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 7)\n",
      "building df: (1000000, 7) 30690\n",
      "(1000000, 7)\n",
      "building df: (2000000, 7) 56309\n",
      "(1000000, 7)\n",
      "building df: (3000000, 7) 79813\n",
      "(1000000, 7)\n",
      "building df: (4000000, 7) 97824\n",
      "(1000000, 7)\n",
      "building df: (5000000, 7) 97825\n",
      "(1000000, 7)\n",
      "building df: (6000000, 7) 97825\n",
      "(1000000, 7)\n",
      "building df: (7000000, 7) 97825\n",
      "(1000000, 7)\n",
      "building df: (8000000, 7) 97825\n",
      "(1000000, 7)\n",
      "building df: (9000000, 7) 101156\n",
      "(1000000, 7)\n",
      "building df: (10000000, 7) 113672\n",
      "(1000000, 7)\n",
      "building df: (11000000, 7) 123773\n",
      "(1000000, 7)\n",
      "building df: (12000000, 7) 135410\n",
      "(1000000, 7)\n",
      "building df: (13000000, 7) 151190\n",
      "(1000000, 7)\n",
      "building df: (14000000, 7) 171340\n",
      "(1000000, 7)\n",
      "building df: (15000000, 7) 191217\n",
      "(1000000, 7)\n",
      "building df: (16000000, 7) 204616\n",
      "(1000000, 7)\n",
      "building df: (17000000, 7) 218277\n",
      "(1000000, 7)\n",
      "building df: (18000000, 7) 230259\n",
      "(1000000, 7)\n",
      "building df: (19000000, 7) 237353\n",
      "(1000000, 7)\n",
      "building df: (20000000, 7) 239592\n",
      "(1000000, 7)\n",
      "building df: (21000000, 7) 242561\n",
      "(1000000, 7)\n",
      "building df: (22000000, 7) 248874\n",
      "(1000000, 7)\n",
      "building df: (23000000, 7) 256261\n",
      "(1000000, 7)\n",
      "building df: (24000000, 7) 265167\n",
      "(1000000, 7)\n",
      "building df: (25000000, 7) 278853\n",
      "(1000000, 7)\n",
      "building df: (26000000, 7) 294627\n",
      "(1000000, 7)\n",
      "building df: (27000000, 7) 305378\n",
      "(1000000, 7)\n",
      "building df: (28000000, 7) 317489\n",
      "(1000000, 7)\n",
      "building df: (29000000, 7) 326139\n",
      "(1000000, 7)\n",
      "building df: (30000000, 7) 332889\n",
      "(1000000, 7)\n",
      "building df: (31000000, 7) 338375\n",
      "(1000000, 7)\n",
      "building df: (32000000, 7) 343602\n",
      "(1000000, 7)\n",
      "building df: (33000000, 7) 349404\n",
      "(1000000, 7)\n",
      "building df: (34000000, 7) 356933\n",
      "(1000000, 7)\n",
      "building df: (35000000, 7) 370899\n",
      "(1000000, 7)\n",
      "building df: (36000000, 7) 378341\n",
      "(1000000, 7)\n",
      "building df: (37000000, 7) 388023\n",
      "(1000000, 7)\n",
      "building df: (38000000, 7) 395307\n",
      "(1000000, 7)\n",
      "building df: (39000000, 7) 401391\n",
      "(1000000, 7)\n",
      "building df: (40000000, 7) 406437\n",
      "(1000000, 7)\n",
      "building df: (41000000, 7) 411533\n",
      "(1000000, 7)\n",
      "building df: (42000000, 7) 416359\n",
      "(1000000, 7)\n",
      "building df: (43000000, 7) 421537\n",
      "(1000000, 7)\n",
      "building df: (44000000, 7) 429904\n",
      "(525921, 7)\n",
      "building df: (44525921, 7) 438284\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# i rid of the unnecessary columns, reading little by little (too big to read it all at once)\n",
    "\n",
    "chunksize = 1000000   # number of rows i read each time\n",
    "list_user_ids=[]\n",
    "\n",
    "# header_names = ['folder_id','num_members','folder_creation_date','last_date_DO_NOT_TRUST','user_id','email_domain','num_adds','num_edits','num_dels',\\\n",
    "#                  'major_content_type','major_file_ext','date','field','new_group_total_pubs','new_group_num_papers_last','new_group_num_citations']\n",
    "header_names = ['folder_id','folder_creation_date','user_id','num_adds','num_edits','num_dels','date']\n",
    "\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "input_file='DROPBOX_duplicates_removed_no_header.csv'     #results_nico2.csv\n",
    "\n",
    "df_folder_user=pd.DataFrame(columns=header_names)  # i create an empty dataframe\n",
    "dict_user_basic_attr={}\n",
    "\n",
    "\n",
    "for chunk_df in pd.read_csv(path+input_file, na_values=[\"NAN\",\"-1\",\"null\"],header=None,  names=header_names,usecols=[0,2,4,6,7,8,11], chunksize=chunksize, sep=None ):\n",
    " \n",
    "    print chunk_df.shape#, len(lista)\n",
    "     \n",
    "    df_folder_user=pd.concat([df_folder_user, chunk_df])  # i add new chunks of data every time, but also drop new duplicates\n",
    "  \n",
    "    print \"building df:\",df_folder_user.shape, len(df_folder_user.user_id.unique())                   \n",
    "    \n",
    "\n",
    "print \"done!\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written: /home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/duplicates_removed_no_header_only_activity_not_aggr.csv \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44525921, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_name=path+input_file.replace(\".csv\",\"\")+.replace(\"_no_header\",\"\")+\"_only_activity_not_aggr.csv\"\n",
    "df_folder_user.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_folder_user.to_csv(new_name, sep=',')\n",
    "print \"written:\", new_name,\"\\n\\n\\n\"\n",
    "df_folder_user.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1206837 entries, 1289 to 525914\n",
      "Data columns (total 7 columns):\n",
      "folder_id               1206837 non-null float64\n",
      "folder_creation_date    1206837 non-null object\n",
      "user_id                 1206837 non-null float64\n",
      "num_adds                1180300 non-null float64\n",
      "num_edits               1180300 non-null float64\n",
      "num_dels                1180300 non-null float64\n",
      "date                    1180300 non-null object\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 73.7+ MB\n",
      "(1206837, 7)\n"
     ]
    }
   ],
   "source": [
    "df_folder_user.info()\n",
    "print df_folder_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44525921, 7)\n",
      "(1206837, 7)\n"
     ]
    }
   ],
   "source": [
    "#df_folder_user.shape\n",
    "\n",
    "#pd.show_versions()\n",
    "\n",
    "#########  OJO!!! THIS IS A SELECTION OF USERS\n",
    "#########  OJO!!! THIS IS A SELECTION OF USERS\n",
    "#########  OJO!!! THIS IS A SELECTION OF USERS\n",
    "##########  OJO!!! THIS IS A SELECTION OF USERS\n",
    "#########  OJO!!! THIS IS A SELECTION OF USERS\n",
    "#########  OJO!!! THIS IS A SELECTION OF USERS\n",
    "#########  OJO!!! THIS IS A SELECTION OF USERS\n",
    "#########  OJO!!! THIS IS A SELECTION OF USERS\n",
    "########  OJO!!! THIS IS A SELECTION OF USERS\n",
    "\n",
    "\n",
    "print df_folder_user.shape   #(44525921, 7)\n",
    "df_folder_user= df_folder_user[df_folder_user['user_id'].isin(list_section_users)]\n",
    "print df_folder_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>folder_id</th>\n",
       "      <th>folder_creation_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_adds</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>num_dels</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2017-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-03-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016-11-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-09-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2017-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-09-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-12-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-07-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-05-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-04-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-12-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2017-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-12-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-12-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-04-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-12-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2017-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-10-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-02-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2016-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2017-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>594826.0</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>76604.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  folder_id folder_creation_date  user_id  num_adds  num_edits  \\\n",
       "0            0   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "1            1   594826.0           2008-06-27  76604.0       0.0        1.0   \n",
       "2            2   594826.0           2008-06-27  76604.0       1.0        3.0   \n",
       "3            3   594826.0           2008-06-27  76604.0       2.0      130.0   \n",
       "4            4   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "5            5   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "6            6   594826.0           2008-06-27  76604.0       1.0        9.0   \n",
       "7            7   594826.0           2008-06-27  76604.0      11.0        0.0   \n",
       "8            8   594826.0           2008-06-27  76604.0       2.0        0.0   \n",
       "9            9   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "10          10   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "11          11   594826.0           2008-06-27  76604.0       2.0        0.0   \n",
       "12          12   594826.0           2008-06-27  76604.0       0.0        0.0   \n",
       "13          13   594826.0           2008-06-27  76604.0      56.0        0.0   \n",
       "14          14   594826.0           2008-06-27  76604.0       0.0        8.0   \n",
       "15          15   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "16          16   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "17          17   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "18          18   594826.0           2008-06-27  76604.0       6.0        0.0   \n",
       "19          19   594826.0           2008-06-27  76604.0       5.0        0.0   \n",
       "20          20   594826.0           2008-06-27  76604.0       1.0        1.0   \n",
       "21          21   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "22          22   594826.0           2008-06-27  76604.0       2.0        4.0   \n",
       "23          23   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "24          24   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "25          25   594826.0           2008-06-27  76604.0       1.0        1.0   \n",
       "26          26   594826.0           2008-06-27  76604.0       3.0        0.0   \n",
       "27          27   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "28          28   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "29          29   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "..         ...        ...                  ...      ...       ...        ...   \n",
       "70          70   594826.0           2008-06-27  76604.0       0.0        1.0   \n",
       "71          71   594826.0           2008-06-27  76604.0       3.0       10.0   \n",
       "72          72   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "73          73   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "74          74   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "75          75   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "76          76   594826.0           2008-06-27  76604.0      70.0        0.0   \n",
       "77          77   594826.0           2008-06-27  76604.0       2.0        0.0   \n",
       "78          78   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "79          79   594826.0           2008-06-27  76604.0       0.0        8.0   \n",
       "80          80   594826.0           2008-06-27  76604.0       2.0       15.0   \n",
       "81          81   594826.0           2008-06-27  76604.0       3.0        0.0   \n",
       "82          82   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "83          83   594826.0           2008-06-27  76604.0       6.0        0.0   \n",
       "84          84   594826.0           2008-06-27  76604.0       8.0        0.0   \n",
       "85          85   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "86          86   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "87          87   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "88          88   594826.0           2008-06-27  76604.0       1.0        1.0   \n",
       "89          89   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "90          90   594826.0           2008-06-27  76604.0       4.0        0.0   \n",
       "91          91   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "92          92   594826.0           2008-06-27  76604.0      11.0        0.0   \n",
       "93          93   594826.0           2008-06-27  76604.0       2.0        0.0   \n",
       "94          94   594826.0           2008-06-27  76604.0       0.0        2.0   \n",
       "95          95   594826.0           2008-06-27  76604.0       2.0       10.0   \n",
       "96          96   594826.0           2008-06-27  76604.0       1.0        0.0   \n",
       "97          97   594826.0           2008-06-27  76604.0       3.0        0.0   \n",
       "98          98   594826.0           2008-06-27  76604.0       7.0        0.0   \n",
       "99          99   594826.0           2008-06-27  76604.0       0.0      146.0   \n",
       "\n",
       "    num_dels        date  \n",
       "0        1.0  2017-04-23  \n",
       "1        0.0  2017-01-13  \n",
       "2        1.0  2016-08-09  \n",
       "3        2.0  2016-01-24  \n",
       "4        1.0  2017-04-23  \n",
       "5        0.0  2016-09-18  \n",
       "6        1.0  2017-03-24  \n",
       "7        5.0  2016-10-31  \n",
       "8        1.0  2015-10-06  \n",
       "9        0.0  2017-04-01  \n",
       "10       0.0  2015-10-07  \n",
       "11       2.0  2016-12-05  \n",
       "12       1.0  2016-11-14  \n",
       "13      54.0  2017-04-04  \n",
       "14       0.0  2016-01-21  \n",
       "15       0.0  2016-06-30  \n",
       "16       0.0  2016-01-02  \n",
       "17       0.0  2016-04-24  \n",
       "18       4.0  2016-03-20  \n",
       "19       3.0  2016-11-22  \n",
       "20       0.0  2016-02-03  \n",
       "21       0.0  2016-12-04  \n",
       "22       0.0  2016-10-29  \n",
       "23       0.0  2016-04-11  \n",
       "24       1.0  2016-11-14  \n",
       "25       1.0  2016-02-02  \n",
       "26       0.0  2016-03-11  \n",
       "27       1.0  2016-02-03  \n",
       "28       1.0  2016-04-11  \n",
       "29       1.0  2015-09-10  \n",
       "..       ...         ...  \n",
       "70       0.0  2015-09-04  \n",
       "71       3.0  2017-02-21  \n",
       "72       0.0  2016-03-06  \n",
       "73       0.0  2016-09-09  \n",
       "74       0.0  2016-12-24  \n",
       "75       0.0  2016-07-10  \n",
       "76       0.0  2016-05-25  \n",
       "77       1.0  2016-04-19  \n",
       "78       1.0  2016-05-01  \n",
       "79       0.0  2016-12-14  \n",
       "80       0.0  2016-11-25  \n",
       "81       1.0  2016-08-09  \n",
       "82       0.0  2016-12-21  \n",
       "83       3.0  2017-01-30  \n",
       "84       0.0  2015-12-22  \n",
       "85       0.0  2016-12-23  \n",
       "86       0.0  2016-04-11  \n",
       "87       1.0  2016-04-20  \n",
       "88       1.0  2016-01-28  \n",
       "89       0.0  2015-12-22  \n",
       "90       0.0  2016-03-18  \n",
       "91       0.0  2015-09-15  \n",
       "92       5.0  2017-02-08  \n",
       "93       2.0  2016-10-17  \n",
       "94       0.0  2017-01-15  \n",
       "95       1.0  2017-01-07  \n",
       "96       0.0  2017-02-20  \n",
       "97       3.0  2016-08-03  \n",
       "98       7.0  2017-04-04  \n",
       "99       0.0  2016-01-19  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folder_user.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "# input_file='duplicates_removed_only_activity_not_aggr.csv'\n",
    "# df_folder_user=pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False, nrows=1000000) # set header=0 if i wanna pass it my own list of header names\n",
    "# print df_folder_user.shape      #  33144, 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial time: 1508355751.62\n",
      "\n",
      "It has been 771.34287715 seconds since the loop started\n",
      "1206837 174872\n",
      "done\n",
      "original df: (1206837, 7) 10000     aggr act by date: (174872, 7) 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#to add up all activity for a given user, folder and date (diff activity counts for diff. types of files (not majoyr_content necessarily), and i aggregate them all)\n",
    "\n",
    "def  grouping(input_df):  # input_df is a selection of all rows for a given user_id and folder_id\n",
    "    \n",
    "\n",
    "    header_names = ['folder_id','folder_creation_date','user_id','num_adds','num_edits','num_dels','date']\n",
    "    df_result = pd.DataFrame(columns=header_names)   \n",
    "\n",
    "    \n",
    "    user_id =  input_df.user_id.iloc[0]\n",
    "    folder_id= input_df.folder_id.iloc[0]\n",
    "    folder_creation_date= input_df.folder_creation_date.iloc[0]\n",
    "\n",
    "    list_dates= list(input_df.date.unique())  # list of unique dates for a given user and folder\n",
    "\n",
    "\n",
    "    for date_act in list_dates:        \n",
    "        \n",
    "          \n",
    "        sum_adds=np.nan\n",
    "        sum_edits=np.nan\n",
    "        sum_dels=np.nan\n",
    "\n",
    "                                          \n",
    "      \n",
    "        if pd.isnull(date_act):\n",
    "#             print date_act, \"oh-oh !\"\n",
    "           \n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "\n",
    "\n",
    "            df_date_selection= input_df[ input_df['date']== date_act]\n",
    "#             print len(df_date_selection),\n",
    "            sum_adds=sum(list(df_date_selection.num_adds.values))\n",
    "            sum_edits=sum(list(df_date_selection.num_edits.values))\n",
    "            sum_dels=sum(list(df_date_selection.num_dels.values))\n",
    "\n",
    "            \n",
    "#         except ValueError:  # if date is NaN\n",
    "#             pass\n",
    "\n",
    "                                                                                                         \n",
    "        df_one_date = pd.DataFrame({'user_id':[user_id],'folder_id':[folder_id],'folder_creation_date':[folder_creation_date],'date':[date_act],'num_adds':[sum_adds],\\\n",
    "                                    'num_edits':[sum_edits],'num_dels':[sum_dels]})                         \n",
    "        df_result = pd.concat([df_result,df_one_date])  # i concatenate all rows for a given user and folder (one row per diff date)              \n",
    "   \n",
    "                         \n",
    "#     print \"after processing:\"\n",
    "#     print df_result.sort_values(by='date')\n",
    "#     raw_input()\n",
    "    return df_result.sort_values(by='date')                     \n",
    "\n",
    "\n",
    "          \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "######################### \n",
    "##########################\n",
    "\n",
    "header_names = ['folder_id','folder_creation_date','user_id','num_adds','num_edits','num_dels','date']\n",
    "  \n",
    "df_aggr_act = pd.DataFrame(columns=header_names)   # i create an empty dataframe\n",
    "\n",
    "cont_tot=0\n",
    "cont_selection=0\n",
    "\n",
    "\n",
    "program_starts = time.time()\n",
    "print \"initial time:\",program_starts\n",
    "\n",
    "\n",
    "\n",
    "cont=0    \n",
    "#for user_id in tqdm_notebook(dict_user_id_list_folders):           \n",
    "for user_id in tqdm_notebook(list_section_users):           \n",
    "    \n",
    "    list_folders=dict_user_id_list_folders[user_id]\n",
    "    \n",
    "    pre_df_selection_user= df_folder_user[df_folder_user['user_id'] == user_id]\n",
    "    \n",
    "    for folder_id in list_folders:\n",
    "        \n",
    "        df_selection =  pre_df_selection_user[ pre_df_selection_user['folder_id'] == folder_id]\n",
    "                                          \n",
    "        cont_tot += len(df_selection)\n",
    "           \n",
    "\n",
    "        df_result= grouping(df_selection)\n",
    "        df_aggr_act = pd.concat([df_aggr_act,df_result])\n",
    "        \n",
    "        cont_selection += len(df_result)\n",
    "        \n",
    "        \n",
    "#     cont +=1    \n",
    "#     if cont == 10000   or  cont == 100000    or    cont == 200000  or  cont == 300000  or cont == 400000 :   \n",
    "#         path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "#         new_name=path+inpu     \n",
    "#     cont +=1\n",
    "#     print cont\n",
    "#     if cont >=10000:        \n",
    "#          breakt_file.replace(\".csv\",\"\")+\"_only_aggr_activity_by_date_PARTIAL__faster.csv\"\n",
    "#         df_aggr_act.to_csv(new_name, sep=',')\n",
    "#         print \"written:\", new_name,\"\\n\\n\\n\"\n",
    "#         df_aggr_act.shape\n",
    "#         break\n",
    "        \n",
    "#     cont +=1\n",
    "#     print cont\n",
    "#     if cont >=10000:        \n",
    "#          break\n",
    "        \n",
    "  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "now = time.time()\n",
    "print(\"It has been {0} seconds since the loop started\".format(now - program_starts))\n",
    "    \n",
    "print cont_tot, cont_selection        \n",
    "print \"done\"  \n",
    "print \"original df:\", df_folder_user.shape,len(df_folder_user.user_id.unique()), \"    aggr act by date:\", df_aggr_act.shape, len(df_aggr_act.user_id.unique()) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "# new_name=path+input_file.replace(\".csv\",\"\")+\"_only_aggr_activity_by_date_PARTIAL.csv\"\n",
    "# df_aggr_act.to_csv(new_name, sep=',')\n",
    "# print \"written:\", new_name,\"\\n\\n\\n\"\n",
    "# df_aggr_act.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder_id</th>\n",
       "      <th>folder_creation_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_adds</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>num_dels</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43662</th>\n",
       "      <td>4.331558e+06</td>\n",
       "      <td>2009-10-25</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46787</th>\n",
       "      <td>4.331558e+06</td>\n",
       "      <td>2009-10-25</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860714</th>\n",
       "      <td>4.331558e+06</td>\n",
       "      <td>2009-10-25</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276351</th>\n",
       "      <td>2.759797e+08</td>\n",
       "      <td>2013-05-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26836</th>\n",
       "      <td>3.104733e+08</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709232</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709207</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-07-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709247</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709173</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709185</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-10-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709195</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-10-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709164</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709214</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709249</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709170</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709211</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709212</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-11-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709252</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2015-12-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709176</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709179</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709187</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709188</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709190</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709210</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709220</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709231</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709248</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709250</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709251</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709227</th>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990324</th>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-06-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990325</th>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990322</th>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990313</th>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990318</th>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990314</th>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990312</th>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990316</th>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95831</th>\n",
       "      <td>7.124049e+08</td>\n",
       "      <td>2014-09-21</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798462</th>\n",
       "      <td>7.153134e+08</td>\n",
       "      <td>2014-09-24</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895109</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173505</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2015-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173508</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173500</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173506</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173507</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173503</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173504</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173502</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173501</th>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920252</th>\n",
       "      <td>9.156533e+08</td>\n",
       "      <td>2015-05-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920253</th>\n",
       "      <td>9.156533e+08</td>\n",
       "      <td>2015-05-20</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448177</th>\n",
       "      <td>9.226667e+08</td>\n",
       "      <td>2015-05-27</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979087</th>\n",
       "      <td>1.005371e+09</td>\n",
       "      <td>2015-09-01</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381107</th>\n",
       "      <td>1.039343e+09</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-10-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381108</th>\n",
       "      <td>1.039343e+09</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83940</th>\n",
       "      <td>1.087149e+09</td>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346853</th>\n",
       "      <td>1.228464e+09</td>\n",
       "      <td>2016-05-31</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145648</th>\n",
       "      <td>1.324476e+09</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103100</th>\n",
       "      <td>1.369219e+09</td>\n",
       "      <td>2016-12-07</td>\n",
       "      <td>2984645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           folder_id folder_creation_date    user_id  num_adds  num_edits  \\\n",
       "43662   4.331558e+06           2009-10-25  2984645.0       NaN        NaN   \n",
       "46787   4.331558e+06           2009-10-25  2984645.0       NaN        NaN   \n",
       "860714  4.331558e+06           2009-10-25  2984645.0       NaN        NaN   \n",
       "276351  2.759797e+08           2013-05-14  2984645.0       NaN        NaN   \n",
       "26836   3.104733e+08           2013-07-22  2984645.0       NaN        NaN   \n",
       "709232  3.547671e+08           2013-10-14  2984645.0       0.0        1.0   \n",
       "709207  3.547671e+08           2013-10-14  2984645.0       0.0        1.0   \n",
       "709247  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709173  3.547671e+08           2013-10-14  2984645.0       0.0        1.0   \n",
       "709185  3.547671e+08           2013-10-14  2984645.0       1.0        2.0   \n",
       "709195  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709164  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709214  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709249  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709170  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709211  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709212  3.547671e+08           2013-10-14  2984645.0       0.0        1.0   \n",
       "709252  3.547671e+08           2013-10-14  2984645.0       0.0        0.0   \n",
       "709176  3.547671e+08           2013-10-14  2984645.0       4.0        0.0   \n",
       "709179  3.547671e+08           2013-10-14  2984645.0       3.0        0.0   \n",
       "709187  3.547671e+08           2013-10-14  2984645.0       5.0        0.0   \n",
       "709188  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709190  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709210  3.547671e+08           2013-10-14  2984645.0       1.0        2.0   \n",
       "709220  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709231  3.547671e+08           2013-10-14  2984645.0       5.0        0.0   \n",
       "709248  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709250  3.547671e+08           2013-10-14  2984645.0       2.0        0.0   \n",
       "709251  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "709227  3.547671e+08           2013-10-14  2984645.0       1.0        0.0   \n",
       "...              ...                  ...        ...       ...        ...   \n",
       "990324  7.118828e+08           2014-09-20  2984645.0       1.0        0.0   \n",
       "990325  7.118828e+08           2014-09-20  2984645.0       0.0        1.0   \n",
       "990322  7.118828e+08           2014-09-20  2984645.0       0.0        1.0   \n",
       "990313  7.118828e+08           2014-09-20  2984645.0       0.0        1.0   \n",
       "990318  7.118828e+08           2014-09-20  2984645.0       0.0        2.0   \n",
       "990314  7.118828e+08           2014-09-20  2984645.0       0.0        1.0   \n",
       "990312  7.118828e+08           2014-09-20  2984645.0       1.0        0.0   \n",
       "990316  7.118828e+08           2014-09-20  2984645.0       0.0        1.0   \n",
       "95831   7.124049e+08           2014-09-21  2984645.0       NaN        NaN   \n",
       "798462  7.153134e+08           2014-09-24  2984645.0       NaN        NaN   \n",
       "895109  7.265170e+08           2014-10-08  2984645.0       NaN        NaN   \n",
       "173505  7.265170e+08           2014-10-08  2984645.0       0.0        0.0   \n",
       "173508  7.265170e+08           2014-10-08  2984645.0       0.0        0.0   \n",
       "173500  7.265170e+08           2014-10-08  2984645.0       1.0        0.0   \n",
       "173506  7.265170e+08           2014-10-08  2984645.0       1.0        0.0   \n",
       "173507  7.265170e+08           2014-10-08  2984645.0      15.0        0.0   \n",
       "173503  7.265170e+08           2014-10-08  2984645.0       1.0        0.0   \n",
       "173504  7.265170e+08           2014-10-08  2984645.0       4.0        0.0   \n",
       "173502  7.265170e+08           2014-10-08  2984645.0       1.0        0.0   \n",
       "173501  7.265170e+08           2014-10-08  2984645.0       1.0        0.0   \n",
       "920252  9.156533e+08           2015-05-20  2984645.0       7.0        0.0   \n",
       "920253  9.156533e+08           2015-05-20  2984645.0       1.0        0.0   \n",
       "448177  9.226667e+08           2015-05-27  2984645.0       NaN        NaN   \n",
       "979087  1.005371e+09           2015-09-01  2984645.0       NaN        NaN   \n",
       "381107  1.039343e+09           2015-10-09  2984645.0       0.0        1.0   \n",
       "381108  1.039343e+09           2015-10-09  2984645.0       0.0        1.0   \n",
       "83940   1.087149e+09           2015-12-06  2984645.0       NaN        NaN   \n",
       "346853  1.228464e+09           2016-05-31  2984645.0       NaN        NaN   \n",
       "145648  1.324476e+09           2016-10-03  2984645.0       NaN        NaN   \n",
       "103100  1.369219e+09           2016-12-07  2984645.0       NaN        NaN   \n",
       "\n",
       "        num_dels        date  \n",
       "43662        NaN         NaN  \n",
       "46787        NaN         NaN  \n",
       "860714       NaN         NaN  \n",
       "276351       NaN         NaN  \n",
       "26836        NaN         NaN  \n",
       "709232       0.0  2015-05-17  \n",
       "709207       0.0  2015-07-02  \n",
       "709247       0.0  2015-08-31  \n",
       "709173       0.0  2015-09-28  \n",
       "709185       0.0  2015-10-09  \n",
       "709195       0.0  2015-10-11  \n",
       "709164       0.0  2015-11-14  \n",
       "709214       0.0  2015-11-14  \n",
       "709249       0.0  2015-11-14  \n",
       "709170       0.0  2015-11-23  \n",
       "709211       0.0  2015-11-23  \n",
       "709212       0.0  2015-11-28  \n",
       "709252      10.0  2015-12-21  \n",
       "709176       0.0  2016-01-06  \n",
       "709179       0.0  2016-01-06  \n",
       "709187       0.0  2016-01-06  \n",
       "709188       0.0  2016-01-06  \n",
       "709190       0.0  2016-01-06  \n",
       "709210       0.0  2016-01-06  \n",
       "709220       0.0  2016-01-06  \n",
       "709231       0.0  2016-01-06  \n",
       "709248       0.0  2016-01-06  \n",
       "709250       0.0  2016-01-06  \n",
       "709251       0.0  2016-01-06  \n",
       "709227       1.0  2016-02-09  \n",
       "...          ...         ...  \n",
       "990324       0.0  2016-06-06  \n",
       "990325       0.0  2016-08-08  \n",
       "990322       0.0  2017-01-24  \n",
       "990313       0.0  2017-01-29  \n",
       "990318       0.0  2017-02-08  \n",
       "990314       0.0  2017-02-13  \n",
       "990312       0.0  2017-03-15  \n",
       "990316       0.0  2017-03-15  \n",
       "95831        NaN         NaN  \n",
       "798462       NaN         NaN  \n",
       "895109       NaN         NaN  \n",
       "173505      25.0  2015-10-18  \n",
       "173508       1.0  2015-10-18  \n",
       "173500       0.0  2016-03-03  \n",
       "173506       0.0  2016-03-03  \n",
       "173507       0.0  2016-03-03  \n",
       "173503       0.0  2016-03-09  \n",
       "173504       0.0  2016-03-09  \n",
       "173502       0.0  2016-11-27  \n",
       "173501       0.0  2017-03-15  \n",
       "920252       0.0  2015-05-20  \n",
       "920253       0.0  2015-05-20  \n",
       "448177       NaN         NaN  \n",
       "979087       NaN         NaN  \n",
       "381107       0.0  2015-10-09  \n",
       "381108       0.0  2015-10-15  \n",
       "83940        NaN         NaN  \n",
       "346853       NaN         NaN  \n",
       "145648       NaN         NaN  \n",
       "103100       NaN         NaN  \n",
       "\n",
       "[136 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for item in df_folder_user.user_id.unique(): #1053958.0 1054719.0 1751354.0 1068548.0 1061513.0 2984645.0 6305527.0 4195383.0 4216344.0 4212072.0 4209407.0 4208321.0 4372500.0 112219835.0 4216163.0 6308205.0 20972493.0 80756063.0 7349394.0 5243437.0 5250955.0 74464510.0 21847065.0 6312378.0 25170299.0 10493708.0 7356996.0 7357288.0 4204271.0 7343853.0 30587102.0 7693230.0 22028635.0 5250770.0 7355085.0 8412120.0 1057827.0 5253799.0 8397434.0 8410242.0 9449891.0 5263869.0 12593657.0 1055449.0 5252667.0 10495736.0 116320344.0 11540129.0 11545882.0 12599171.0 10503885.0 4217850.0 35665917.0 44041341.0 11546544.0 11537171.0 11554324.0 11553745.0 4216144.0 632298388.0 6311602.0 11546164.0 10485817.0 5246433.0 12396010.0 12589034.0 12592619.0 12588657.0 664817197.0 1520945.0 5250768.0 10313887.0 19576770.0 10493560.0 191907825.0 12582939.0 13644692.0 13643706.0 2107779.0 13807527.0 4208865.0 16796895.0 10507223.0 1050439.0 12586415.0 2099524.0 6311443.0 14688997.0 14696161.0 14698022.0 18885673.0 14699608.0 16785414.0 13654556.0 14696878.0 37758348.0 15379790.0 764\n",
    "#     print item,\n",
    "\n",
    "user_id=2984645\n",
    "\n",
    "df_folder_user[df_folder_user[\"user_id\"] == user_id].sort_values(by=['folder_id','date'])#.folder_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>folder_creation_date</th>\n",
       "      <th>folder_id</th>\n",
       "      <th>num_adds</th>\n",
       "      <th>num_dels</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-10-25</td>\n",
       "      <td>4.331558e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-05-14</td>\n",
       "      <td>2.759797e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-07-22</td>\n",
       "      <td>3.104733e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-17</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-09-28</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-11</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-14</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-23</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-28</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-21</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-09</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-10</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-14</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-17</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-13</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-02</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-03</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-10</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-17</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-28</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-09-06</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>3.547671e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-06-21</td>\n",
       "      <td>2014-09-19</td>\n",
       "      <td>7.109204e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-08-05</td>\n",
       "      <td>2014-09-19</td>\n",
       "      <td>7.109204e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-15</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-21</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-05-04</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-06-06</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-08-08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-24</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>2014-09-20</td>\n",
       "      <td>7.118828e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-09-21</td>\n",
       "      <td>7.124049e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-09-24</td>\n",
       "      <td>7.153134e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-03</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-09</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-11-27</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>2014-10-08</td>\n",
       "      <td>7.265170e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-20</td>\n",
       "      <td>2015-05-20</td>\n",
       "      <td>9.156533e+08</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-27</td>\n",
       "      <td>9.226667e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-01</td>\n",
       "      <td>1.005371e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>1.039343e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-15</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>1.039343e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.087149e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-05-31</td>\n",
       "      <td>1.228464e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>1.324476e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-07</td>\n",
       "      <td>1.369219e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2984645.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date folder_creation_date     folder_id  num_adds  num_dels  \\\n",
       "0          NaN           2009-10-25  4.331558e+06       NaN       NaN   \n",
       "0          NaN           2013-05-14  2.759797e+08       NaN       NaN   \n",
       "0          NaN           2013-07-22  3.104733e+08       NaN       NaN   \n",
       "0   2015-05-17           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2015-07-02           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2015-08-31           2013-10-14  3.547671e+08       1.0       0.0   \n",
       "0   2015-09-28           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2015-10-09           2013-10-14  3.547671e+08       1.0       0.0   \n",
       "0   2015-10-11           2013-10-14  3.547671e+08       1.0       0.0   \n",
       "0   2015-11-14           2013-10-14  3.547671e+08       3.0       0.0   \n",
       "0   2015-11-23           2013-10-14  3.547671e+08       2.0       0.0   \n",
       "0   2015-11-28           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2015-12-21           2013-10-14  3.547671e+08       0.0      10.0   \n",
       "0   2016-01-06           2013-10-14  3.547671e+08      25.0       0.0   \n",
       "0   2016-02-09           2013-10-14  3.547671e+08       1.0       1.0   \n",
       "0   2016-02-10           2013-10-14  3.547671e+08       1.0       0.0   \n",
       "0   2016-03-21           2013-10-14  3.547671e+08      26.0       0.0   \n",
       "0   2016-04-14           2013-10-14  3.547671e+08       2.0       4.0   \n",
       "0   2016-04-17           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2016-04-22           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2016-07-13           2013-10-14  3.547671e+08       0.0       1.0   \n",
       "0   2016-08-02           2013-10-14  3.547671e+08       9.0       0.0   \n",
       "0   2016-08-03           2013-10-14  3.547671e+08       1.0       0.0   \n",
       "0   2016-08-05           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2016-08-10           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2016-08-17           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2016-08-28           2013-10-14  3.547671e+08      11.0       5.0   \n",
       "0   2016-09-06           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2016-09-13           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "0   2016-09-25           2013-10-14  3.547671e+08       0.0       0.0   \n",
       "..         ...                  ...           ...       ...       ...   \n",
       "0   2015-06-21           2014-09-19  7.109204e+08       0.0       0.0   \n",
       "0   2015-08-05           2014-09-19  7.109204e+08       1.0       0.0   \n",
       "0   2016-02-15           2014-09-20  7.118828e+08       0.0       0.0   \n",
       "0   2016-02-21           2014-09-20  7.118828e+08      15.0       0.0   \n",
       "0   2016-04-13           2014-09-20  7.118828e+08       0.0       0.0   \n",
       "0   2016-05-04           2014-09-20  7.118828e+08       0.0       0.0   \n",
       "0   2016-06-06           2014-09-20  7.118828e+08       1.0       0.0   \n",
       "0   2016-08-08           2014-09-20  7.118828e+08       0.0       0.0   \n",
       "0   2017-01-24           2014-09-20  7.118828e+08       0.0       0.0   \n",
       "0   2017-01-29           2014-09-20  7.118828e+08       0.0       0.0   \n",
       "0   2017-02-08           2014-09-20  7.118828e+08       0.0       0.0   \n",
       "0   2017-02-13           2014-09-20  7.118828e+08       0.0       0.0   \n",
       "0   2017-03-15           2014-09-20  7.118828e+08       1.0       0.0   \n",
       "0          NaN           2014-09-21  7.124049e+08       NaN       NaN   \n",
       "0          NaN           2014-09-24  7.153134e+08       NaN       NaN   \n",
       "0          NaN           2014-10-08  7.265170e+08       NaN       NaN   \n",
       "0   2015-10-18           2014-10-08  7.265170e+08       0.0      26.0   \n",
       "0   2016-03-03           2014-10-08  7.265170e+08      17.0       0.0   \n",
       "0   2016-03-09           2014-10-08  7.265170e+08       5.0       0.0   \n",
       "0   2016-11-27           2014-10-08  7.265170e+08       1.0       0.0   \n",
       "0   2017-03-15           2014-10-08  7.265170e+08       1.0       0.0   \n",
       "0   2015-05-20           2015-05-20  9.156533e+08       8.0       0.0   \n",
       "0          NaN           2015-05-27  9.226667e+08       NaN       NaN   \n",
       "0          NaN           2015-09-01  1.005371e+09       NaN       NaN   \n",
       "0   2015-10-09           2015-10-09  1.039343e+09       0.0       0.0   \n",
       "0   2015-10-15           2015-10-09  1.039343e+09       0.0       0.0   \n",
       "0          NaN           2015-12-06  1.087149e+09       NaN       NaN   \n",
       "0          NaN           2016-05-31  1.228464e+09       NaN       NaN   \n",
       "0          NaN           2016-10-03  1.324476e+09       NaN       NaN   \n",
       "0          NaN           2016-12-07  1.369219e+09       NaN       NaN   \n",
       "\n",
       "    num_edits    user_id  \n",
       "0         NaN  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         2.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         2.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         7.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         3.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         2.0  2984645.0  \n",
       "..        ...        ...  \n",
       "0         1.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         2.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         0.0  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         1.0  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "0         NaN  2984645.0  \n",
       "\n",
       "[76 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aggr_act[df_aggr_act[\"user_id\"] == user_id].sort_values(by=['folder_id','date'])#.folder_id.unique()8.76751745e+08,   6.85855622e+08,   1.06067770e+09,\n",
    "#          2.20767048e+08,   5.20016200e+07,   8.55651348e+08,\n",
    "#          9.01756795e+08,   4.06968508e+08,   1.94927800e+06])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original df: (44525921, 7) 438284 519045     aggr act by date: (69275, 7) 534 8792\n"
     ]
    }
   ],
   "source": [
    "print \"original df:\", df_folder_user.shape,len(df_folder_user.user_id.unique()),len(df_folder_user.folder_id.unique()), \"    aggr act by date:\", df_aggr_act.shape, len(df_aggr_act.user_id.unique()),len(df_filtered.folder_id.unique()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written: /home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_user_id_tot_act_PARTIAL.pickle\n"
     ]
    }
   ],
   "source": [
    "dict_user_id_tot_act={}\n",
    "for user_id in tqdm_notebook(df_folder_user.user_id.unique()):  # i use the original df to get ALL users (even those with only nans in activity dates)\n",
    "    df_select = df_aggr_act[df_aggr_act['user_id']== user_id]\n",
    "    \n",
    "    sum_act=sum(list(df_select.num_adds.values)) + sum(list(df_select.num_edits.values)) + sum(list(df_select.num_dels.values))  # if the selected df is empty, the sum is 0 (not nan)\n",
    "    \n",
    "    dict_user_id_tot_act[user_id]= sum_act\n",
    "    \n",
    "#     if len(df_select) == 0:\n",
    "#         print user_id, df_select\n",
    "#         print \"tot act:\",sum_act\n",
    "#         raw_input()\n",
    "        \n",
    "\n",
    "    \n",
    "pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_user_id_tot_act_PARTIAL.pickle'\n",
    "with open(pickle_name, 'wb') as handle:\n",
    "    pickle.dump(dict_user_id_tot_act, handle)\n",
    "print \"written:\", pickle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juliaponcela/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/juliaponcela/anaconda2/lib/python2.7/site-packages/tqdm/_tqdm.py\", line 103, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/juliaponcela/anaconda2/lib/python2.7/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written: /home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_user_id_list_folders.pickle\n"
     ]
    }
   ],
   "source": [
    "dict_user_id_list_folders={}\n",
    "for user_id in tqdm_notebook(df_folder_user.user_id.unique()):  # i use the original df to get ALL users (even those with only nans in activity dates)\n",
    "    \n",
    "    df_select = df_folder_user[df_folder_user['user_id']== user_id]  \n",
    "    \n",
    "    \n",
    "    dict_user_id_list_folders[user_id]=list(df_select.folder_id.unique())\n",
    "    \n",
    "    \n",
    "    \n",
    "pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_user_id_list_folders.pickle'\n",
    "with open(pickle_name, 'wb') as handle:\n",
    "    pickle.dump(dict_user_id_list_folders, handle)\n",
    "print \"written:\", pickle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juliaponcela/anaconda2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/juliaponcela/anaconda2/lib/python2.7/site-packages/tqdm/_tqdm.py\", line 103, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/juliaponcela/anaconda2/lib/python2.7/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written: /home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_folder_id_list_users.pickle\n"
     ]
    }
   ],
   "source": [
    "dict_folder_id_list_users={}\n",
    "for folder_id in tqdm_notebook(df_folder_user.folder_id.unique()):  # i use the original df to get ALL users (even those with only nans in activity dates)\n",
    "    \n",
    "    df_select = df_folder_user[df_folder_user['folder_id']== folder_id]      \n",
    "    dict_folder_id_list_users[folder_id]=list(df_select.user_id.unique())\n",
    "    \n",
    "    \n",
    "    \n",
    "pickle_name='/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/dict_folder_id_list_users.pickle'\n",
    "with open(pickle_name, 'wb') as handle:\n",
    "    pickle.dump(dict_folder_id_list_users, handle)\n",
    "print \"written:\", pickle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8079"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_folder_id_list_users)   # users: 438284     dict_folder_id_list_users: 8079"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477158296.0 3.0\n",
      "90357909.0 159.0\n",
      "1325389.0 355689.0\n",
      "1344786.0 1250.0\n",
      "498373072.0 69.0\n",
      "326914.0 268467.0\n",
      "2499235.0 78406.0\n",
      "714981.0 754.0\n",
      "1661512.0 322.0\n",
      "598398887.0 30.0\n",
      "712528.0 4272.0\n",
      "2917469.0 1934.0\n",
      "336512669.0 30.0\n",
      "60732587.0 652.0\n"
     ]
    }
   ],
   "source": [
    "#df_aggr_act[df_aggr_act['user_id'] == 596461291]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder_id</th>\n",
       "      <th>folder_creation_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_adds</th>\n",
       "      <th>num_edits</th>\n",
       "      <th>num_dels</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51178</th>\n",
       "      <td>5.903474e+06</td>\n",
       "      <td>2009-12-28</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51179</th>\n",
       "      <td>5.903474e+06</td>\n",
       "      <td>2009-12-28</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54303</th>\n",
       "      <td>5.903474e+06</td>\n",
       "      <td>2009-12-28</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54304</th>\n",
       "      <td>5.903474e+06</td>\n",
       "      <td>2009-12-28</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68230</th>\n",
       "      <td>5.903474e+06</td>\n",
       "      <td>2009-12-28</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68231</th>\n",
       "      <td>5.903474e+06</td>\n",
       "      <td>2009-12-28</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39870</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39871</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39872</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39873</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39874</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39875</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39876</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39877</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-02-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39878</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39879</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39880</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39881</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39882</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39883</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2017-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39884</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39885</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39886</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39887</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-02-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39888</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39889</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-05-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39890</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39891</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39892</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39893</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40073</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40074</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40075</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40076</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40077</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40078</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40079</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40080</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40081</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40082</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40083</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40084</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40085</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40086</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40087</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40088</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40089</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40090</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40091</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40092</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40093</th>\n",
       "      <td>1.290651e+09</td>\n",
       "      <td>2016-08-21</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74689</th>\n",
       "      <td>1.448647e+09</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74690</th>\n",
       "      <td>1.448647e+09</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74691</th>\n",
       "      <td>1.448647e+09</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74692</th>\n",
       "      <td>1.448647e+09</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74693</th>\n",
       "      <td>1.448647e+09</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74694</th>\n",
       "      <td>1.448647e+09</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74695</th>\n",
       "      <td>1.448647e+09</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13286</th>\n",
       "      <td>7.017424e+06</td>\n",
       "      <td>2010-02-03</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78638</th>\n",
       "      <td>8.074914e+08</td>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>596461291.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-03-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          folder_id folder_creation_date      user_id  num_adds  num_edits  \\\n",
       "51178  5.903474e+06           2009-12-28  596461291.0       1.0        0.0   \n",
       "51179  5.903474e+06           2009-12-28  596461291.0       0.0        1.0   \n",
       "54303  5.903474e+06           2009-12-28  596461291.0       1.0        0.0   \n",
       "54304  5.903474e+06           2009-12-28  596461291.0       0.0        1.0   \n",
       "68230  5.903474e+06           2009-12-28  596461291.0       1.0        0.0   \n",
       "68231  5.903474e+06           2009-12-28  596461291.0       0.0        1.0   \n",
       "39870  1.290651e+09           2016-08-21  596461291.0       0.0       12.0   \n",
       "39871  1.290651e+09           2016-08-21  596461291.0       0.0        2.0   \n",
       "39872  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "39873  1.290651e+09           2016-08-21  596461291.0       0.0        0.0   \n",
       "39874  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "39875  1.290651e+09           2016-08-21  596461291.0       0.0        6.0   \n",
       "39876  1.290651e+09           2016-08-21  596461291.0       0.0        7.0   \n",
       "39877  1.290651e+09           2016-08-21  596461291.0       1.0        0.0   \n",
       "39878  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "39879  1.290651e+09           2016-08-21  596461291.0       1.0       28.0   \n",
       "39880  1.290651e+09           2016-08-21  596461291.0       2.0        1.0   \n",
       "39881  1.290651e+09           2016-08-21  596461291.0       1.0        0.0   \n",
       "39882  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "39883  1.290651e+09           2016-08-21  596461291.0       2.0        1.0   \n",
       "39884  1.290651e+09           2016-08-21  596461291.0       1.0        1.0   \n",
       "39885  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "39886  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "39887  1.290651e+09           2016-08-21  596461291.0       1.0        3.0   \n",
       "39888  1.290651e+09           2016-08-21  596461291.0       0.0       12.0   \n",
       "39889  1.290651e+09           2016-08-21  596461291.0       0.0       12.0   \n",
       "39890  1.290651e+09           2016-08-21  596461291.0       0.0        0.0   \n",
       "39891  1.290651e+09           2016-08-21  596461291.0       0.0        4.0   \n",
       "39892  1.290651e+09           2016-08-21  596461291.0       0.0       12.0   \n",
       "39893  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "...             ...                  ...          ...       ...        ...   \n",
       "40073  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "40074  1.290651e+09           2016-08-21  596461291.0       0.0       18.0   \n",
       "40075  1.290651e+09           2016-08-21  596461291.0       0.0       26.0   \n",
       "40076  1.290651e+09           2016-08-21  596461291.0       0.0        0.0   \n",
       "40077  1.290651e+09           2016-08-21  596461291.0       0.0        0.0   \n",
       "40078  1.290651e+09           2016-08-21  596461291.0       0.0        4.0   \n",
       "40079  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "40080  1.290651e+09           2016-08-21  596461291.0       0.0        6.0   \n",
       "40081  1.290651e+09           2016-08-21  596461291.0       1.0        4.0   \n",
       "40082  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "40083  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "40084  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "40085  1.290651e+09           2016-08-21  596461291.0       0.0        2.0   \n",
       "40086  1.290651e+09           2016-08-21  596461291.0       0.0        0.0   \n",
       "40087  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "40088  1.290651e+09           2016-08-21  596461291.0       0.0        1.0   \n",
       "40089  1.290651e+09           2016-08-21  596461291.0       0.0       18.0   \n",
       "40090  1.290651e+09           2016-08-21  596461291.0       0.0       16.0   \n",
       "40091  1.290651e+09           2016-08-21  596461291.0       0.0        2.0   \n",
       "40092  1.290651e+09           2016-08-21  596461291.0       1.0        1.0   \n",
       "40093  1.290651e+09           2016-08-21  596461291.0       0.0        2.0   \n",
       "74689  1.448647e+09           2017-04-08  596461291.0       1.0        1.0   \n",
       "74690  1.448647e+09           2017-04-08  596461291.0       0.0        4.0   \n",
       "74691  1.448647e+09           2017-04-08  596461291.0       1.0        0.0   \n",
       "74692  1.448647e+09           2017-04-08  596461291.0       1.0        0.0   \n",
       "74693  1.448647e+09           2017-04-08  596461291.0       0.0        3.0   \n",
       "74694  1.448647e+09           2017-04-08  596461291.0       1.0        0.0   \n",
       "74695  1.448647e+09           2017-04-08  596461291.0       0.0        6.0   \n",
       "13286  7.017424e+06           2010-02-03  596461291.0       NaN        NaN   \n",
       "78638  8.074914e+08           2015-01-16  596461291.0       1.0        0.0   \n",
       "\n",
       "       num_dels        date  \n",
       "51178       1.0  2017-04-30  \n",
       "51179       0.0  2017-04-29  \n",
       "54303       1.0  2017-04-30  \n",
       "54304       0.0  2017-04-29  \n",
       "68230       1.0  2017-04-30  \n",
       "68231       0.0  2017-04-29  \n",
       "39870       0.0  2017-03-10  \n",
       "39871       0.0  2017-02-13  \n",
       "39872       0.0  2017-04-06  \n",
       "39873       1.0  2017-04-03  \n",
       "39874       0.0  2017-04-09  \n",
       "39875       0.0  2017-03-26  \n",
       "39876       0.0  2017-04-27  \n",
       "39877       0.0  2017-02-11  \n",
       "39878       0.0  2017-02-13  \n",
       "39879       0.0  2017-03-26  \n",
       "39880       0.0  2017-03-05  \n",
       "39881       1.0  2017-05-14  \n",
       "39882       0.0  2017-05-03  \n",
       "39883       2.0  2017-03-14  \n",
       "39884       0.0  2017-03-08  \n",
       "39885       0.0  2017-03-06  \n",
       "39886       0.0  2017-04-11  \n",
       "39887       0.0  2017-02-14  \n",
       "39888       0.0  2017-03-10  \n",
       "39889       0.0  2017-05-15  \n",
       "39890       1.0  2017-04-03  \n",
       "39891       0.0  2017-03-12  \n",
       "39892       0.0  2017-04-17  \n",
       "39893       1.0  2017-04-03  \n",
       "...         ...         ...  \n",
       "40073       0.0  2017-04-05  \n",
       "40074       0.0  2017-03-15  \n",
       "40075       0.0  2017-03-09  \n",
       "40076       1.0  2017-04-03  \n",
       "40077       1.0  2017-03-28  \n",
       "40078       0.0  2017-05-11  \n",
       "40079       0.0  2017-05-10  \n",
       "40080       0.0  2017-04-07  \n",
       "40081       0.0  2017-04-01  \n",
       "40082       0.0  2017-04-07  \n",
       "40083       0.0  2017-04-06  \n",
       "40084       0.0  2017-03-06  \n",
       "40085       0.0  2017-05-10  \n",
       "40086       1.0  2017-03-25  \n",
       "40087       0.0  2017-03-27  \n",
       "40088       0.0  2017-04-14  \n",
       "40089       0.0  2017-04-29  \n",
       "40090       0.0  2017-04-09  \n",
       "40091       0.0  2017-04-12  \n",
       "40092       0.0  2017-03-04  \n",
       "40093       0.0  2017-04-02  \n",
       "74689       0.0  2017-04-09  \n",
       "74690       0.0  2017-04-10  \n",
       "74691       0.0  2017-05-14  \n",
       "74692       1.0  2017-04-09  \n",
       "74693       0.0  2017-04-10  \n",
       "74694       1.0  2017-04-10  \n",
       "74695       0.0  2017-04-09  \n",
       "13286       NaN         NaN  \n",
       "78638       1.0  2017-03-10  \n",
       "\n",
       "[239 rows x 7 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folder_user[df_folder_user['user_id'] == 596461291]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # unpack each original row into as many rows as authors in the paper:\n",
    "\n",
    "# def process(row):\n",
    "    \n",
    "#     aids = [int(a) for a in row.author_id.split('|')] # list of author ids in that row\n",
    "#     n = len(aids)\n",
    "#     names = row.author_name.split('|')  # list of author names in that row\n",
    "#     affil = []\n",
    "#     if pd.isnull(row.affiliation):   # if there is no affiliation info, then affil is a list of n nans\n",
    "#         affil = [np.nan]*n\n",
    "#     else:\n",
    "#         affil_list = row.affiliation.split('|') # if there is affiliation info, i build a list of affiliations                       \n",
    "        \n",
    "#         for idx in row.idx.split('|'): ## list of affiliations indexes for each author , idx is a list of affil indexes for one author                                                           \n",
    "            \n",
    "            \n",
    "#             if idx=='-1':\n",
    "#                 affil.append('?')\n",
    "#                 # OR:\n",
    "#                 # affil.append(row.affiliation)\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     affil.append('|'.join([affil_list[int(i)] for i in idx.split(',')]))                               \n",
    "                \n",
    "                \n",
    "#                 except IndexError: \n",
    "#                     affil.append('?')\n",
    "   \n",
    "\n",
    "    \n",
    "#     #return pd.DataFrame({'uid':[row.uid]*n,'author_id':aids,'author_name':names,'affiliation':affil,'seq':range(len(aids))})\n",
    "    \n",
    "#     # instead, i do:\n",
    "#     return pd.DataFrame({'uid':[row.uid]*n,'author_id':aids,'author_name':names,'affiliation':affil,'seq':range(len(aids))})\n",
    "\n",
    "\n",
    "# result = pd.concat([process(row[1]) for row in  df.iterrows() ] )   # row is a tuple row_index, the entire row info (searchable by:  row[1].author_id  )\n",
    "# #result = pd.concat([process(row[1]) for row in  tqdm_notebook(df.iterrows(), desc='1st loop')   ]     )   # row is a tuple row_index, the entire row info (searchable by:  row[1].author_id  )\n",
    "# print \"done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(df_user.email_domain.unique())  #  7478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rural Urban Continuum Code\n",
    "\n",
    "# 1\tCounties in metro areas of 1 million population or more\tMetro Counties\n",
    "# 2\tCounties in metro areas of 250,000 to 1 million population\tMetro Counties\n",
    "# 3\tCounties in metro areas of fewer than 250,000 population\tMetro Counties\n",
    "# 4\tUrban population of 20,000 or more, adjacent to a metro area\tNonmetro Counties\n",
    "# 5\tUrban population of 20,000 or more, not adjacent to a metro area\tNonmetro Counties\n",
    "# 6\tUrban population of 2,500 to 19,999, adjacent to a metro area\tNonmetro Counties\n",
    "# 7\tUrban population of 2,500 to 19,999, not adjacent to a metro area\tNonmetro Counties\n",
    "# 8\tCompletely rural or less than 2,500 urban population, adjacent to a metro area\tNonmetro Counties\n",
    "# 9\tCompletely rural or less than 2,500 urban population, not adjacent to a metro area\tNonmetro Counties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "# input_file='new_DROPBOX.csv'\n",
    "\n",
    "\n",
    "# df_merged.drop('University_Name', axis=1, inplace=True)\n",
    "# df_merged.drop('simplified_domain', axis=1, inplace=True)\n",
    "# df_merged.rename(columns={'Cleaned_university_name': 'University_name'}, inplace=True)\n",
    "\n",
    "\n",
    "# new_name=path+input_file.replace(\".csv\",\"\")+\"_basic_user_attr.csv\"\n",
    "# df_merged.to_csv(new_name, sep=',')\n",
    "# print \"written:\", new_name,\"\\n\\n\\n\"\n",
    "# df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df_merged.columns\n",
    "#print len(df_merged.folder_id.unique()) , \"unique folders\"#   417657  folders\n",
    "print len(df_merged.user_id.unique()) , \"unique users\"  # 345043  users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_user=df_merged\n",
    "\n",
    "# df_user.drop('folder_id', axis=1, inplace=True)\n",
    "# df_user.drop('num_members', axis=1, inplace=True)\n",
    "# df_user.drop('folder_creation_date', axis=1, inplace=True)\n",
    "# df_user.drop('num_adds', axis=1, inplace=True)\n",
    "# df_user.drop('num_edits', axis=1, inplace=True)\n",
    "# df_user.drop('num_dels', axis=1, inplace=True)\n",
    "# df_user.drop('date', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print df_user.shape   #  (9018863, 16)    (9018883, 7)\n",
    "df_user=df_user.drop_duplicates()  # i have dropped them from the shell already\n",
    "print df_user.shape , \"after dropping duplicates\"  # (349767, 7)\n",
    "print len(df_user.user_id.unique()) , \"unique users\"  # 345043  users\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(df_user.user_id.unique()) , \"unique users\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_user[df_user['user_id']== 273811289]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_merged[df_merged[\"Cleaned_university_name\"]==\"northwestern university\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_complete_info=df_original[['folder_id','group_total_publ',  'num_edits' ]].dropna(how='any')  # by default, dropna drops a row if ANY  of the indicated fields are missing (i can also set it to if ALL the fields are missing: how='all')\n",
    "# print df_complete_info.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #'folder_id','folder_creation_date' ,'date_last_change','user_id' ,'email_domain'   1427672  (all rows)\n",
    "\n",
    "# #'folder_id','num_adds' ,'num_edits', 'num_deletes'    501103\n",
    "\n",
    "\n",
    "# # 'folder_id','group_total_publ' , 'group_num_papers_last','group_num_citations'  318989\n",
    "\n",
    "# # 'folder_id','group_total_publ',  'num_edits'     123330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "new_name=path+\"df_new_dropbox_users.csv\"\n",
    "df_user.to_csv(new_name, sep=',')\n",
    "print \"written:\", new_name,\"\\n\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_merged = df_merged.drop('simplified_domain_x', 1)\n",
    "#df_merged = df_merged.drop('simplified_domain_y', 1)\n",
    "#df_merged.rename(columns={'email_domain_x': 'email_domain'}, inplace=True)\n",
    "#df_final=df_merged.drop_duplicates(subset=[\"folder_id\",\"user_id\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_final['University_Country'] = df_final.apply(lambda row: str(row.Cleaned_university_name) + \", \" + str(row.Country) , axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_university_country (input_df):\n",
    "    \n",
    "#     univ_country = str(input_df.Cleaned_university_name)+ \", \" + str(input_df.Country)\n",
    "# #     print row\n",
    "# #     print univ_country\n",
    "# #     raw_input()\n",
    "\n",
    "#     return univ_country\n",
    "\n",
    "# df_merged['University_Country'] = df_merged.apply (lambda row: get_university_country(row),axis=1)\n",
    "\n",
    "# name=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/Dropbox_datafile_may22_2017_modified_added_univ_country.csv\"\n",
    "# df_merged.to_csv(name, sep=',')\n",
    "# print \"written:\", name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#### i combine the incomplete dict and online queries:\n",
    "\n",
    "try:\n",
    "    path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/\"\n",
    "    filename=\"dict_univ_country_geolocation_incomplete.pickle\"\n",
    "\n",
    "    file = open(path+filename,'r')\n",
    "    dict_univ_country_geolocation = pickle.load(file)\n",
    "    len(dict_univ_country_geolocation.keys())\n",
    "    \n",
    "    print \"partial dict found:\", filename\n",
    "    \n",
    "except:\n",
    "    dict_univ_country_geolocation={} \n",
    "    print \"dict not found, starting from scratch\"\n",
    "\n",
    "\n",
    "try:\n",
    "        \n",
    "        path=\"/home/julia/Dropbox_collaborations/Results/\"\n",
    "        filename=\"list_places_with_ZERO_or_UNKNOWN_RESULTS.pickle\"\n",
    "        file = open(path+filename,'r')\n",
    "        list_places_with_zero_or_unknown_results = pickle.load(file)\n",
    "        print \"partial dict found:\", filename\n",
    "\n",
    "except :\n",
    "        list_places_with_zero_or_unknown_results=[]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "url_string=\"http://maps.googleapis.com/maps/api/geocode/json?address=\"  # this way i access the google api that gives me directly the location of any address or city etc\n",
    "\n",
    "\n",
    "# url_string=\"https://maps.googleapis.com/maps/api/geocode/json?address=\"  # this way i access the google api that gives me directly the location of any address or city etc\n",
    "# my_google_api_key=\"AIzaSyDtkJYPIxK-DD-st-dhwHlIfvhvH2-CCVU\"   #\"AIzaSyBVg6GL7j3eYRpv_8rHVaT5jIHr8TGIt0Q\" #  \"AIzaSyBMpsaefERA6sXv2UGUwK53foT12p0aObA\"   #'AIzaSyAE8V_dc9UpVZFGZghUtRlU8lapq0Fa63w'\n",
    "\n",
    "\n",
    "\n",
    "#time.sleep(54000)  # wait for 15h\n",
    "tot_attempts=4\n",
    "\n",
    "list_not_found=[]\n",
    "\n",
    "def get_geolocation (input_df):\n",
    "    \n",
    "  \n",
    "    #string_location=input_df.University_Country\n",
    "    string_location=str(input_df.Cleaned_university_name)+ \", \" + str(input_df.Country)\n",
    "\n",
    "\n",
    "    try:\n",
    "        latlong= dict_univ_country_geolocation[string_location]  # if i have already calculated that geolocation before\n",
    "        #print \"geolocation already in dict\"\n",
    "    except KeyError:\n",
    "\n",
    "        latlong = np.nan\n",
    "\n",
    "        try:\n",
    "\n",
    "            attempts = 0\n",
    "            success = False\n",
    "\n",
    "            while success != True  and  attempts < tot_attempts: \n",
    "                #data = requests.get(url_string+string_location+\"&key=\"+my_google_api_key  )  # i make the request for a given site\n",
    "\n",
    "                data = requests.get(url_string+string_location  )  # i make the request for a given site\n",
    "                jdict = json.loads(data.text)        # it conveninently parses the info using the fact that it is in JSON (it is like a big dictionary of dictionaries)                                            \n",
    "\n",
    "                status=jdict['status']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if status == \"OVER_QUERY_LIMIT\":  # if too many queries per second\n",
    "                    time.sleep(5)    # waiting time in seconds\n",
    "                    success = False   \n",
    "                    attempts +=1\n",
    "\n",
    "                    if attempts == tot_attempts:\n",
    "                      # send an alert as this means that the daily limit has been reached\n",
    "\n",
    "                        print \"Daily limit has been reached\"\n",
    "                        print jdict525\n",
    "\n",
    "                        path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/\"\n",
    "                        filename=\"dict_univ_country_geolocation_incomplete_.pickle\"\n",
    "                        with open(path+filename,'wb') as f:\n",
    "                            pickle.dump(dict_univ_country_geolocation, f)\n",
    "                        print \"written:\",path+filename                                                                        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/\"\n",
    "                        filename=\"list_places_with_ZERO_or_UNKNOWN_RESULTS_incomplete.pickle\"\n",
    "                        with open(path+filename,'wb') as f:\n",
    "                            pickle.dump(list_places_with_zero_or_unknown_results, f)\n",
    "                        print \"written:\",path+filename, \"!!\"\n",
    "\n",
    "                                                                                                                        \n",
    "                        \n",
    "                        \n",
    "                        print \"\\nhit ctrl+C to kill the process   :(\"\n",
    "                        raw_input()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                elif status=='OK':\n",
    "\n",
    "                        zip_code=\"\"\n",
    "                        \n",
    "                        lat = jdict['results'][0]['geometry']['location']['lat']\n",
    "                        lng = jdict['results'][0]['geometry']['location']['lng']\n",
    "\n",
    "\n",
    "                        try:\n",
    "                            \n",
    "                            for item in  jdict['results'][0]['address_components']:\n",
    "                                if item['types']== ['postal_code']:\n",
    "                                    zip_code=item['long_name']                                    \n",
    "                                   \n",
    "                                    break\n",
    "                            \n",
    "                              #zip_code = jdict['results'][0]['address_components'][-1]['long_name']      this sometimes access other info that is not the zip code!\n",
    "                            \n",
    "\n",
    "                        except IndexError:                            \n",
    "                            print jdict\n",
    "                            raw_input()\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "\n",
    "                        latlong = (lat, lng)                 \n",
    "                        latlong=str(latlong)+\" \"+zip_code                            \n",
    "                        dict_univ_country_geolocation[string_location]=latlong     \n",
    "\n",
    "\n",
    "\n",
    "                        success = True \n",
    "\n",
    "\n",
    "                        #print string_location, dict_univ_country_geolocation[string_location],latlong\n",
    "\n",
    "                else:# if it cant find the geolocation of the string :      'status': u'ZERO_RESULTS',  OR  'status': u'UNKNOWN_ERROR'\n",
    "                   \n",
    "                    list_places_with_zero_or_unknown_results.append(string_location)\n",
    "\n",
    "\n",
    "\n",
    "                    print jdict , string_location,\"added  to list of places with zero or unknown results\"\n",
    "                    success = True \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        except KeyError:  # if it cant find the geolocation of the string\n",
    "\n",
    "            print string_location,\"couldnt be found\"\n",
    "            print jdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return latlong\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "#########################\n",
    "# df_merged['Geolocation'] = df_merged.apply (lambda row: get_geolocation(row, dict_univ_country_geolocation,tot_attempts,list_places_with_zero_or_unknown_results),axis=1)\n",
    "\n",
    "%time df_merged['Geolocation'] = df_merged.apply (lambda row: get_geolocation(row),axis=1)\n",
    "print \"done!\\n\"\n",
    "\n",
    "print \"len of dict:\",len(dict_univ_country_geolocation.keys())\n",
    "\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/\"\n",
    "filename=\"dict_univ_country_geolocation_YAY.pickle\"\n",
    "with open(path+filename,'wb') as f:\n",
    "    pickle.dump(dict_univ_country_geolocation, f)\n",
    "print \"written:\",path+filename, \"!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorted(dict_univ_country_geolocation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_univ_country_geolocation['northwestern university, United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dict_univ_country_geolocation.keys())# 3424   total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_keys_to_pop=[]\n",
    "for key in dict_univ_country_geolocation:\n",
    "    try:\n",
    "        math.isnan(dict_univ_country_geolocation[key])\n",
    "        print key,\"     \", dict_univ_country_geolocation[key]\n",
    "        list_keys_to_pop.append(key)\n",
    "        \n",
    "    except: pass\n",
    "        \n",
    "print len(list_keys_to_pop)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # print len(dict_univ_country_geolocation.keys())\n",
    "# # for key in list_keys_to_pop:\n",
    "    \n",
    "# #     dict_univ_country_geolocation.pop(key)\n",
    "    \n",
    "# print len(dict_univ_country_geolocation.keys())      2487\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### from dictionary (built by running the code at the nicoresearch)\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/\"\n",
    "filename=\"dict_univ_country_geolocation_YAY.pickle\"\n",
    "\n",
    "file = open(path+filename,'r')\n",
    "dict_univ_country_geolocation = pickle.load(file)\n",
    "print len(dict_univ_country_geolocation.keys())     # total: 3425\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_geolocation_by_dict (input_df):\n",
    "    \n",
    "    #string_location=input_df.University_Country\n",
    "    string_location=str(input_df.Cleaned_university_name)+ \", \" + str(input_df.Country)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        latlong= dict_univ_country_geolocation[string_location]  # if i have already calculated that geolocation before\n",
    "        #print \"geolocation already in dict\"\n",
    "    except KeyError:\n",
    "\n",
    "        latlong = np.nan\n",
    "        \n",
    "       \n",
    "            \n",
    "      \n",
    "        \n",
    "    \n",
    "\n",
    "# df=df_merged\n",
    "# for column in df.columns:\n",
    "#     for idx in df[column].index:\n",
    "#         x = df.get_value(idx,column)\n",
    "#         try:\n",
    "#             x = unicode(x.encode('utf-8','ignore'),errors ='ignore') if type(x) == unicode else unicode(str(x),errors='ignore')\n",
    "#             df.set_value(idx,column,x)\n",
    "#         except Exception:\n",
    "#             print 'encoding error: {0} {1}'.format(idx,column)\n",
    "#             df.set_value(idx,column,'')\n",
    "#             continue\n",
    "\n",
    "\n",
    "    return latlong\n",
    "\n",
    "\n",
    "%time df_merged['Geolocation'] = df_merged.apply (lambda row: get_geolocation_by_dict(row),axis=1)\n",
    "print \"done\"\n",
    "\n",
    "print \"len of dict:\",len(dict_univ_country_geolocation.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_zip(cadena):    # example of cadena:   (42.0564594, -87.67526699999999) 60208\n",
    "    \n",
    "    try:\n",
    "        new_cadena=cadena.split(\") \")[1]        \n",
    "        new_cadena=int(new_cadena)\n",
    "        \n",
    "    except :  # for NANs  OR NAMES\n",
    "        new_cadena=np.nan\n",
    "    return new_cadena\n",
    "######\n",
    "\n",
    "def get_lat (cadena):   # (lat, lng) zip  -----     example:      (40.3439888, -74.6514481) 08544   \n",
    "    try:\n",
    "        new_cadena=cadena.split(\") \")[0].split(\", \")[0].strip(\"(\")\n",
    "    except AttributeError:  # for NANs  (which are Floats, and dont have .slitp!)\n",
    "        new_cadena=np.nan\n",
    "    return new_cadena\n",
    "    \n",
    "######\n",
    "\n",
    "def get_long (cadena):   # (lat, lng)              \n",
    "    try:\n",
    "        new_cadena=cadena.split(\") \")[0].split(\", \")[1]\n",
    "    except AttributeError:  # for NANs  (which are Floats, and dont have .slitp!)\n",
    "        new_cadena=np.nan\n",
    "    return new_cadena\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "df_merged['zip'] = df_merged.Geolocation.apply(get_zip)\n",
    "\n",
    "df_merged['lat'] = df_merged.Geolocation.apply(get_lat)\n",
    "df_merged['long'] = df_merged.Geolocation.apply(get_long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_merged[df_merged[\"Country\"] != \"United States\"].head(200)\n",
    "df_merged[df_merged[\"Cleaned_university_name\"] == \"northwestern university\"].head(2)   #NU: 60208.0   university of miami 33146.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####### i overwrite any zip codes from non-US countries (because i do not have RUCC codes nor density for them)\n",
    "df_no_international_ZIP= df_merged\n",
    "df_no_international_ZIP.ix[df_no_international_ZIP.Country != \"United States\" , 'zip'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df_merged.shape\n",
    "print df_no_international_ZIP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged[df_merged[\"Country\"] != \"United States\"].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_no_international_ZIP[df_no_international_ZIP[\"zip\"] == np.nan]\n",
    "\n",
    "dict_university_zip_code={}\n",
    "list_indexes_no_zip=[]\n",
    "for index, row in df_no_international_ZIP.iterrows():\n",
    "    try: \n",
    "        int(row['zip'])\n",
    "        dict_university_zip_code[row['Cleaned_university_name']]=row['zip']\n",
    "    except ValueError:   # if the zip is a NAN    \n",
    "        list_indexes_no_zip.append(index)\n",
    "    \n",
    "\n",
    "df_no_international_ZIP.ix[list_indexes_no_zip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_ZIP=df_no_international_ZIP.ix[list_indexes_no_zip]\n",
    "df_no_ZIP_US=df_no_ZIP[df_no_ZIP[\"Country\"] == \"United States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for item in df_no_ZIP_US.Cleaned_university_name.unique():\n",
    "    print item\n",
    "\n",
    "    \n",
    "dict_missing_univ_zip_codes={ 'university of north carolina at chapel hill' :27599, 'university of illinois' :61820 ,'rutgers university': 8901,'tufts university'  : 2153,'texas a&m university' :  77840, 'washington state university'  : 99164,'university of north carolina' : 27599, 'university of illinois urbana-champaign' : 61820,'buffalo state' : 14222 , 'north carolina a&t state university' : 27411,'texas a&m university-commerce' : 75428, 'alabama a&m university' : 35762, 'west texas a&m university' : 79016,'st georges university' : 11739,'prairie view a&m university' : 77446,'adler university'  : 60602,'texas a&m international university' : 78041,'texas a&m university-texarkana' : 75503,'town of boone' : 28608}\n",
    "\n",
    "## OJO!! python does not accept values that start with 0   !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_university_zip_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_missing_zip(university_name):#, dict_missing_univ_zip_codes,dict_university_zip_code):\n",
    "    \n",
    "#     zip_code=np.nan\n",
    "    \n",
    "#     try:\n",
    "#         zip_code=dict_missing_univ_zip_codes[university_name]\n",
    "#     except KeyError:\n",
    "#          zip_code=dict_university_zip_code[university_name]\n",
    "            \n",
    "#     return zip_code\n",
    "\n",
    "    \n",
    "    \n",
    "# df_no_international_ZIP['new_zip'] = df_no_international_ZIP.Cleaned_university_name.apply(get_missing_zip)#, dict_missing_univ_zip_codes,dict_university_zip_code)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_missing_zip(row, dict_missing_univ_zip_codes,dict_university_zip_code):\n",
    "    \n",
    "    zip_code=np.nan\n",
    "    university_name=row[\"Cleaned_university_name\"]\n",
    "    try:\n",
    "        zip_code=dict_missing_univ_zip_codes[university_name]\n",
    "    except KeyError:\n",
    "        try:\n",
    "             zip_code=dict_university_zip_code[university_name]\n",
    "        except KeyError: pass\n",
    "            \n",
    "    return zip_code\n",
    "\n",
    "    \n",
    "    \n",
    "df_no_international_ZIP['new_zip'] = df_no_international_ZIP.apply(lambda row: get_missing_zip(row, dict_missing_univ_zip_codes,dict_university_zip_code),axis=1)#.Cleaned_university_name.apply(get_missing_zip)#, dict_missing_univ_zip_codes,dict_university_zip_code)\n",
    "    \n",
    "\n",
    "#df_merged['Geolocation'] = df_merged.apply (lambda row: get_geolocation(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no_international_ZIP.ix[list_indexes_no_zip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no_international_ZIP.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_no_international_ZIP = df_no_international_ZIP.drop('zip', 1)\n",
    "df_no_international_ZIP.rename(columns={'new_zip': 'zip'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# there are encoding issues, so i need to run this before i write the dataframe to csv:\n",
    "\n",
    "df=df_no_international_ZIP\n",
    "for column in df.columns:\n",
    "    for idx in df[column].index:\n",
    "        x = df.get_value(idx,column)\n",
    "        try:\n",
    "            x = unicode(x.encode('utf-8','ignore'),errors ='ignore') if type(x) == unicode else unicode(str(x),errors='ignore')\n",
    "            df.set_value(idx,column,x)\n",
    "        except Exception:\n",
    "            print 'encoding error: {0} {1}'.format(idx,column)\n",
    "            df.set_value(idx,column,'')\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_merge_name=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_YAY.xlsx\"\n",
    "df_no_international_ZIP.to_csv(new_merge_name, sep=';', errors='ignore', encoding='utf-8')\n",
    "\n",
    "# writer = pd.ExcelWriter(new_merge_name, engine='xlsxwriter',options={'strings_to_urls': False})\n",
    "# # Convert the dataframe to an XlsxWriter Excel object.\n",
    "# %time df_merged.to_excel(writer, sheet_name='Sheet1')\n",
    "# ## OJO! hay url muy largas que hacen que el sistema se cuelgue (solucion: no permitirle que las considere url, sino simplemente str)\n",
    "\n",
    "# # Close the Pandas Excel writer and output the Excel file.\n",
    "# writer.save()\n",
    "\n",
    "print \"written:\",new_merge_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_merged[df_merged[\"Cleaned_university_name\"]==\"northwestern university\"][[\"folder_id\",\"lat\",\"long\",'Cleaned_university_name', 'zip','Country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Rural_Urban_zip_codes/\"\n",
    "input_file='Zipcode-ZCTA-Population-Density-And-Area.csv'\n",
    "df_pop_density=pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False) # set header=0 if i wanna pass it my own list of header names\n",
    "print df_pop_density.shape      #  33144, 4\n",
    "\n",
    "\n",
    "                                \n",
    "df_merged_with_zip_rucc = pd.merge(df_no_international_ZIP, df_zip_merged_rucc, how='left',on='zip') \n",
    "print \"merged between master file and zip urban rural file done.\"\n",
    "print df_merged_with_zip_rucc.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_merged_rucc_pop_density = pd.merge(df_merged_with_zip_rucc, df_pop_density, how='left',on='zip') \n",
    "print \"merged between master file with RUCC and pop-density.\"\n",
    "print df_merged_rucc_pop_density.shape\n",
    "\n",
    "\n",
    "\n",
    "new_merge_name=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_population_density_when_available.csv\"\n",
    "df_merged_rucc_pop_density.to_csv(new_merge_name, sep=';', encoding='utf-8')\n",
    "print \"written:\", new_merge_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_population_density_when_available.pickle\"\n",
    "with open(name,'wb') as f:\n",
    "    pickle.dump(df_merged_rucc_pop_density, f)\n",
    "\n",
    "print \"written:\", name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####  i add university ranking\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/University_rankings_and_names/\"\n",
    "# input_file='list_top_100universities.tsv'\n",
    "input_file = \"world_ranking_parsed.csv\"\n",
    "df_univ_ranking=pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"], usecols=[\"world_ranking\",\"national_ranking\",\"University\"]) # set header=0 if i wanna pass it my own list of header names\n",
    "df_univ_ranking.rename(columns={'University': 'Cleaned_university_name'}, inplace=True)\n",
    "\n",
    "print df_univ_ranking.shape      #  33144, \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "input_file='Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_population_density_when_available.csv'\n",
    "df_complete=pd.read_csv(path+input_file, sep=';',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False, parse_dates=['folder_creation_date','date_last_change']) # set header=0 if i wanna pass it my own list of header names\n",
    "df_complete.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "print df_complete.shape      #  33144, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_complete.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_complete, df_univ_ranking, how='left',on='Cleaned_university_name') \n",
    "print \"merged between master file with RUCC and pop-density, and university ranking.\"\n",
    "print df_merged.shape\n",
    "\n",
    "df_merged[[\"ranking_University\", \"Cleaned_university_name\",\"zip\",\"email_domain\",\"world_ranking\",\"national_ranking\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_merged.drop('ranking_University', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "new_merge_name=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_population_density_when_available.csv\"\n",
    "df_merged.to_csv(new_merge_name, sep=';', encoding='utf-8')\n",
    "print \"written:\", new_merge_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_population_density_when_available.pickle\"\n",
    "with open(name,'wb') as f:\n",
    "    pickle.dump(df_merged, f)\n",
    "\n",
    "print \"written:\", name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Rural_Urban_zip_codes/\"\n",
    "# input_file='Zipcode-ZCTA-Population-Density-And-Area.csv'\n",
    "# df_pop_density=pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False) # set header=0 if i wanna pass it my own list of header names\n",
    "# print df_pop_density.shape      #  33144, 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_merged_with_zip_rucc[df_merged_with_zip_rucc[\"Country\"]!=\"United States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_merged_with_zip_rucc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#df_merged_with_zip_rucc[df_merged_with_zip_rucc[\"Cleaned_university_name\"]==\"university of chicago\"][[\"folder_id\",\"lat\",\"long\",'Cleaned_university_name', 'zip','Country','Rural_Urban_Continuum_Code','State_Name','Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_merged_with_zip_rucc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## these are equivalent ways of getting the same counts:\n",
    "\n",
    "df_merged_with_zip_rucc.Rural_Urban_Continuum_Code.value_counts()\n",
    "# 1.0     1185989\n",
    "# 4.0       27778\n",
    "# 7.0       17949\n",
    "# 2.0         891\n",
    "# 5.0         356\n",
    "# 10.0        258\n",
    "# 9.0         152\n",
    "# 8.0          43\n",
    "# 3.0          14\n",
    "\n",
    "\n",
    "df_merged_with_zip_rucc.groupby('Rural_Urban_Continuum_Code').size()    #.sort_values()   from smallest group to largest (instead of from smallest index to largest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##  'folder_id' 1308727\n",
    "##  'Geolocation'   1308727\n",
    "##  'lat'  1308727\n",
    "##  'Rural_Urban_Continuum_Code'   1233430\n",
    "##  'folder_id','Rural_Urban_Continuum_Code'  1233430 \n",
    "##  'State_Name'  1233430\n",
    "##  'folder_id','group_total_publ'    315354\n",
    "##  'folder_id','group_total_publ','num_edits'    121674\n",
    "##  'folder_id','group_total_publ','num_edits','num_adds'   121674\n",
    "##  'folder_id','group_total_publ','num_edits','num_adds','Geolocation'   121674\n",
    "##  'folder_id','group_total_publ','num_edits','num_adds','Geolocation','Rural_Urban_Continuum_Code'    118337\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_complete_info=df_original[['folder_id','group_total_publ',  'num_edits' ]].dropna(how='any')  # by default, dropna drops a row if ANY  of the indicated fields are missing (i can also set it to if ALL the fields are missing: how='all')\n",
    "df_selection=df_merged_with_zip_rucc[['folder_id','Rural_Urban_Continuum_Code']].dropna(how='any')  # by default, dropna drops a row if ANY  of the indicated fields are missing (i can also set it to if ALL the fields are missing: how='all')\n",
    "\n",
    "print df_merged_with_zip_rucc.shape\n",
    "print df_selection.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged_with_zip_rucc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in sorted(dict_univ_country_geolocation.keys()):\n",
    "        print key, \"     ------\",dict_univ_country_geolocation[key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### i add population density to the df:\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Dropbox/\"\n",
    "#input_file='Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_only_US_YAY.pickle'\n",
    "input_file='Dropbox_datafile_may22_2017_modified_added_univ_country_geolocation_RUCC_when_available.pickle'\n",
    "df_merged_with_zip_rucc=pd.read_pickle(path+input_file)\n",
    "print df_merged_with_zip_rucc.shape    #   1403347, 26\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/Rural_Urban_zip_codes/\"\n",
    "input_file='Zipcode-ZCTA-Population-Density-And-Area.csv'\n",
    "df_pop_density=pd.read_csv(path+input_file, sep=',',na_values=[\"NAN\",\"-1\",\"null\"],low_memory=False) # set header=0 if i wanna pass it my own list of header names\n",
    "print df_pop_density.shape      #  33144, 4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_merged_rucc_pop_density = pd.merge(df_merged_with_zip_rucc, df_pop_density, how='left',on='zip') \n",
    "print \"merged between master file with RUCC and pop-density.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_merged_rucc_pop_density.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_merged_with_zip_rucc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Results/\"\n",
    "filename=\"dict_lat_log_aggregate_activity.pickle\"\n",
    "\n",
    "\n",
    "file = open(path+filename,'r')\n",
    "dict_lat_log_aggregate_activity = pickle.load(file)\n",
    "\n",
    "df_master_geoloc_act=pd.DataFrame.from_dict(dict_lat_log_aggregate_activity,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_lat_log_aggregate_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_master_geoloc_act.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "trace1 = Bar(   # instead of Scatter, for big datasets!\n",
    "#     y = df_merged_with_zip_rucc['num_edits'],  #np.random.randn(500),\n",
    "#     x = df_merged_with_zip_rucc['Rural_Urban_Continuum_Code'],  #np.random.randn(500),\n",
    "    \n",
    "    x = df_master_geoloc_act['common_num_publ'],#.head(10000),  #np.random.randn(500),   \n",
    "    y = df_master_geoloc_act['num_edits'],#.head(10000),  #np.random.randn(500),\n",
    "   \n",
    "    \n",
    "    #mode='markers',\n",
    "#     marker=dict(\n",
    "#         size='5',\n",
    "#         color = df_master_geoloc_act['Rural_Urban_Continuum_Code'].head(100), #set color equal to a variable\n",
    "#         colorscale='Viridis',\n",
    "#         showscale=True\n",
    "#     )\n",
    ")\n",
    "data = [trace1]\n",
    "layout = Layout(\n",
    "    #title='Plot Title',\n",
    "    xaxis=dict(\n",
    "        title='Most common number of publications per geolocation',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Number edits',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "#py.iplot(data, filename='scatter-plot-with-colorscale')\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "#fig = { 'data':data, layout:Layout}#, 'layout':layout }\n",
    "iplot(fig, filename='bar_plot_activity_num_publ')\n",
    "offline.plot(fig, filename='/home/juliaponcela/bar_plot_activity_num_publ.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trace1 = Scatter(\n",
    "    y = np.random.randn(500),\n",
    "    x = np.random.randn(500),\n",
    "    \n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size='16',\n",
    "        color = np.random.randn(500), #set color equal to a variable\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    )\n",
    ")\n",
    "data = [trace1]\n",
    "\n",
    "#py.iplot(data, filename='scatter-plot-with-colorscale')\n",
    "\n",
    "\n",
    "fig = { 'data':data}#, 'layout':layout }\n",
    "iplot(fig, filename='scatter-plot-with-colorscale')\n",
    "offline.plot(fig, filename='/home/juliaponcela/scatter-plot-with-colorscale.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print df_merged_with_zip_rucc.Rural_Urban_Continuum_Code.value_counts()\n",
    "#print df_merged_with_zip_density_rucc.zip.value_counts()\n",
    "print df_merged_with_zip_rucc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in sorted(dict_univ_country_geolocation.keys()):\n",
    "    print key, \"     ------\",dict_univ_country_geolocation[key]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged_with_zip_rucc.iloc[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged_with_zip_rucc.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# address=\"Northwestern University\"\n",
    "# address=\"5650+Sheridan+Road+Chicago,+Illinois\"\n",
    "# list_addresses=[\"Bijuesca, Spain\",\"Zaragoza, Spain\",\"Northwestern University\",\"Yale University, New Haven\",\"Universidad de Zaragoza\",\"Princeton University\",\"Uppsala University\",\"Imperial College\",\"greenwich uk\",\"architectural association school of architecture\",\"delhi, india\"]\n",
    "\n",
    "\n",
    "# url_string=\"http://maps.googleapis.com/maps/api/geocode/json?address=\"  # this way i access the google api that gives me directly the location of any address or city etc\n",
    "\n",
    "# for address in list_addresses:\n",
    "#     data = requests.get(url_string+address )  # i make the request for a given site\n",
    "#     jdict = json.loads(data.text)        # i conveninently parse the info using the fact that it is in JSON (it is like a big dictionary of dictionaries)\n",
    "#     try:\n",
    "#         lat = jdict['results'][0]['geometry']['location']['lat']\n",
    "#         lng = jdict['results'][0]['geometry']['location']['lng']\n",
    "#         latlong = (lat, lng)                 \n",
    "#     except:\n",
    "#         latlong = ''\n",
    "#     print address, latlong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# jdict['results'][0]['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #string=\"https://maps.googleapis.com/maps/api/distancematrix/json?origins=Seattle&destinations=Chicago\"   #### or multiple destinations:    destinations=San+Francisco|Victoria+BC     or    origins=Vancouver+BC|Seattle    or  units=imperial   # or origins=40.6655101,-73.89188969999998&destinations=40.6905615%2C-73.9976592\n",
    "#    # or units=metric\n",
    "\n",
    "\n",
    "# coords_1 = (41.878114 , -87.629798)\n",
    "# coords_2 = ( 40.416775,-3.70379 )\n",
    "\n",
    "# print geopy.distance.vincenty(coords_1, coords_2).miles, \"miles,     \", geopy.distance.vincenty(coords_1, coords_2).km, \"km\"\n",
    "# ##########\n",
    "    \n",
    "    \n",
    "# #mi casita en Bijuesca: 41.541835, -1.920094   ### Bijuesca-Zgz   130Km, 80millas\n",
    "# ### casa del yayo  en zgz:   41.647014, -0.883251\n",
    "# ### casa del yayo en bijuesca:   41.539197, -1.km920401\n",
    "# ### Paris 48.8583701,2.2944813           #### Bijuesca-Paris   1060Km,  659millas\n",
    "\n",
    "# #### chicago: 41.878114 , -87.629798\n",
    "\n",
    "\n",
    "# #### madrid: 40.416775,-3.70379       Chicago-Madrid:   6,720 km  4182 miles\n",
    "\n",
    "\n",
    "\n",
    "# # ### i can use google maps to calculate distances, but if the points are in different continents, it doesnt work, because it gives your routes, not just distances!\n",
    "# # string=\"https://maps.googleapis.com/maps/api/distancematrix/json?\"   #### or multiple destinations:    destinations=San+Francisco|Victoria+BC     or    origins=Vancouver+BC|Seattle    or  units=imperial   # or origins=40.6655101,-73.89188969999998&destinations=40.6905615%2C-73.9976592\n",
    "# #    # or units=metric\n",
    "    \n",
    "# # for address1 in list_addresses:  \n",
    "# #     for address2 in list_addresses:\n",
    "# #         if address != address2:\n",
    "            \n",
    "            \n",
    "# #             query=string+\"origins=\"+address1+\"&destinations=\"+address2\n",
    "# #             print address, address2\n",
    "# #             data = requests.get(query )\n",
    "# #             jdict = json.loads(data.text)    \n",
    "# #             print jdict\n",
    "\n",
    "# #             try:                         \n",
    "# #                 distance=float(jdict[\"rows\"][0][\"elements\"][0][\"distance\"][\"text\"].strip(\" km\").replace(\",\",\"\"))\n",
    "# #                 print distance\n",
    "                \n",
    "# #             except :\n",
    "# #                 print \"cant calculate distance  :(\"\n",
    "# #             print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #df_final.Country.count()\n",
    "\n",
    "# #df_final[pd.isnull(df_final).any(axis=1)]\n",
    "\n",
    "\n",
    "# for item in list(df_final[df_final['University_Name'].isnull() ].email_domain.unique()): #####  & (df_final['email_domain'].isnull())\n",
    "#     print item\n",
    "# #df_final[df_final['University_Name'].isnull() ].shape\n",
    "\n",
    "\n",
    "# #### count number of instances for each pair of different possible values for the columns country and univ:\n",
    "\n",
    "# df_final.groupby(['Cleaned_university_name','Country']).size().reset_index().rename(columns={0:'count'})\n",
    "#df_final.Country.count()\n",
    "\n",
    "# #df_final[pd.isnull(df_final).any(axis=1)]\n",
    "\n",
    "\n",
    "# for item in list(df_final[df_final['University_Name'].isnull() ].email_domain.unique()): #####  & (df_final['email_domain'].isnull())\n",
    "#     print item\n",
    "# #df_final[df_final['University_Name'].isnull() ].shape\n",
    "# # print len(df_merged.Cleaned_university_name.unique())\n",
    "\n",
    "\n",
    "# ##### count number of rows with a non-Nan value\n",
    "# #df_merged.count()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### i add the info on university name and country to merge it with the df\n",
    "# #filename_univ=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/University_rankings_and_names/Domain_Uni_Assoc_RH_cleaned.csv\"\n",
    "\n",
    "# filename_univ_old=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/University_rankings_and_names/Domain_Uni_Assoc_RH_cleaned_added_simplified_domains.csv\"\n",
    "# df_univ_names_old=pd.read_csv(filename_univ_old, sep=',',usecols=[\"email_domain\",\"simplified_domain\",\"Cleaned_university_name\",\"University_Name\",\"Country\"])# [email_domain,University_Name,Country] #, header=None)\n",
    "# print df_univ_names_old.shape\n",
    "# ############   OJOOOOO!!! THIS MAPPING FILE HAS ONLY 4k EMAIL DOMAINS, WHILE THE DROPBOX MASTER FILE HAS AROUND 7K UNIQUE EMAIL DOMAINS!!! (I ALREADY EMAILED REBECCA ABOUT IT, JUN 1ST)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# filename_univ_new=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/University_rankings_and_names/NEW_DOMAINS_NICO_Uni_Class_RH_0601.csv\"\n",
    "# df_univ_names_new=pd.read_csv(filename_univ_new, sep=',',usecols=[\"email_domain\",\"University_Name\",\"Country\"])# [email_domain,University_Name,Country] #, header=None)\n",
    "# print df_univ_names_new.shape       \n",
    "# ############   OJOOOOO!!! THIS MAPPING FILE HAS ONLY 4k EMAIL DOMAINS, WHILE THE DROPBOX MASTER FILE HAS AROUND 7K UNIQUE EMAIL DOMAINS!!! (I ALREADY EMAILED REBECCA ABOUT IT, JUN 1ST)\n",
    "# df_univ_names_new.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def lower_univ(cadena):\n",
    "#     try:\n",
    "#         return cadena.lower()\n",
    "#     except:\n",
    "#         return cadena\n",
    "\n",
    "\n",
    "\n",
    "# ##################\n",
    "\n",
    "\n",
    "\n",
    "# df_merged_univ_names = pd.merge(df_univ_names_new, df_univ_names_old, how='left',on='email_domain')\n",
    "\n",
    "\n",
    "\n",
    "# df_merged_univ_names['University_Name_x'] = df_merged_univ_names.University_Name_x.apply(lower_univ)\n",
    "# df_merged_univ_names.drop('Country_y', axis=1, inplace=True)\n",
    "# df_merged_univ_names.drop('simplified_domain', axis=1, inplace=True)   # because some rows from the new file dont have it, so i drop it and calculate it again\n",
    "\n",
    "# df_merged_univ_names.rename(columns={'Country_x': 'Country'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def add_simplified_domain(old_cadena):\n",
    "#     number_pieces=2\n",
    "    \n",
    "#     if old_cadena.split(\".\")[-2]==\"edu\"  or  old_cadena.split(\".\")[-2]==\"ac\" :  # for cases like:    mail.xjtu.edu.cn  or   student.kug.ac.at\n",
    "#         number_pieces=3\n",
    "    \n",
    "#     try:\n",
    "        \n",
    "#         new_cadena='.'.join(old_cadena.split(\".\")[-number_pieces:])\n",
    "# #         print old_cadena,type(old_cadena)\n",
    "# #         print new_cadena,type(new_cadena)\n",
    "# #         raw_input()\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "#         return new_cadena\n",
    "#     except TypeError:  # if it is a string already\n",
    "# #         print type(old_cadena)\n",
    "# #         print old_cadena\n",
    "# #         raw_input()\n",
    "#         return old_cadena\n",
    "\n",
    "\n",
    "\n",
    "# df_merged_univ_names['simplified_domain']  = df_merged_univ_names.email_domain.apply(add_simplified_domain)\n",
    "\n",
    "\n",
    "\n",
    "# #df_final['University_Country'] = df_final.apply(lambda row: str(row.Cleaned_university_name) + \", \" + str(row.Country) , axis=1)\n",
    "\n",
    "\n",
    "# def get_new_university_fill_NANs (input_df):\n",
    "    \n",
    "# #     print  \"\\n\\n\",input_df\n",
    "#     new_univ=input_df.University_Name_x\n",
    "   \n",
    "#     try:\n",
    "#         np.isnan(input_df.Cleaned_university_name)\n",
    "# #         print \"   null!!\"\n",
    "        \n",
    "#     except TypeError:\n",
    "#         new_univ=input_df.Cleaned_university_name\n",
    "        \n",
    "        \n",
    "# #     print row[1]['University_Name_x'] \n",
    "# #     print row[1]['Cleaned_university_name']  \n",
    "# #     print \"NEW CLEANED UNIV:\",new_univ\n",
    "    \n",
    "    \n",
    "# #     raw_input()\n",
    "    \n",
    "#     return new_univ\n",
    "   # #df_merged_univ_names['simplified_domain']  = df_merged_univ_names.email_domain.apply(add_simplified_domain)\n",
    "\n",
    "# #df_merged_univ_names['simplified_domain']  = df_merged_univ_names.email_domain.apply(add_simplified_domain)\n",
    "\n",
    "# df_merged_univ_names['new_Cleaned_university_name'] = df_merged_univ_names.apply(lambda row: get_new_university_fill_NANs(row),axis=1)\n",
    "\n",
    "# #print row:-\n",
    "# # email_domain                                2009.ljmu.ac.uk\n",
    "# # University_Name_x          liverpool john moores university\n",
    "# # Country                                      United Kingdom\n",
    "# # University_Name_y                                       NaN\n",
    "# # Cleaned_university_name                                 NaN\n",
    "# # simplified_domain                                       NaN\n",
    "# # Name: 0, dtype: object\n",
    "\n",
    "\n",
    "\n",
    "# ### to access a particular field, do:  row.email_domain\n",
    "\n",
    "\n",
    "\n",
    "# new_merge_name=\"/home/juliaponcela/at_NICO/Dropbox_collaboration_patterns/Data/University_rankings_and_names/merged_domains_new_unclean.csv\"\n",
    "# df_merged_univ_names.to_csv(new_merge_name, sep=',')\n",
    "# print \"written:\", new_merge_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def add_simplified_domain(old_cadena):\n",
    "#     number_pieces=2\n",
    "    \n",
    "#     if old_cadena.split(\".\")[-2]==\"edu\"  or  old_cadena.split(\".\")[-2]==\"ac\" :  # for cases like:    mail.xjtu.edu.cn  or   student.kug.ac.at\n",
    "#         number_pieces=3\n",
    "    \n",
    "#     try:\n",
    "        \n",
    "#         new_cadena='.'.join(old_cadena.split(\".\")[-number_pieces:])\n",
    "# #         print old_cadena,type(old_cadena)\n",
    "# #         print new_cadena,type(new_cadena)\n",
    "# #         raw_input()\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "#         return new_cadena\n",
    "#     except TypeError:  # if it is a string already\n",
    "# #         print type(old_cadena)\n",
    "# #         print old_cadena\n",
    "# #         raw_input()\n",
    "#         return old_cadena\n",
    "\n",
    "\n",
    "# df_original['simplified_domain']  = df_original.email_domain.apply(add_simplified_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# dict_domain_ranking=df_univ_names.set_index('University_linkedin')['group_ranking30_University'].to_dict()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "d9909cdbd8ce4da6a60cee20fa54f5f5": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
